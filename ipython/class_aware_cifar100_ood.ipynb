{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec7ea42",
   "metadata": {},
   "source": [
    "# Class-Aware OOD with CIFAR-100\n",
    "This walkthrough configures MetaLoRA for class-aware sampling on CIFAR-100 and evaluates out-of-distribution robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84dabe1",
   "metadata": {},
   "source": [
    "- Configure paths, dependencies, and experiment settings.\n",
    "3- Train on all CIFAR-100 classes with a class-aware sampler.\n",
    "- Use SVHN as a truly OOD benchmark while reusing the CIFAR-trained head.\n",
    "- Report both in-distribution accuracy and SVHN OOD metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78698751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100 as TorchvisionCIFAR100, SVHN as TorchvisionSVHN\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path, marker: str = \"main.py\", max_depth: int = 10) -> Path:\n",
    "    \"\"\"Locate the repository root when running locally or inside Colab.\"\"\"\n",
    "    env_root = os.environ.get(\"METALORA_ROOT\") or os.environ.get(\"REPO_ROOT\")\n",
    "    if env_root:\n",
    "        candidate = Path(env_root).expanduser().resolve()\n",
    "        if (candidate / marker).exists():\n",
    "            return candidate\n",
    "    current = start.resolve()\n",
    "    for _ in range(max_depth):\n",
    "        if (current / marker).exists():\n",
    "            return current\n",
    "        if current.parent == current:\n",
    "            break\n",
    "        current = current.parent\n",
    "    colab_candidate = Path(\"/content/metalora\").resolve()\n",
    "    if (colab_candidate / marker).exists():\n",
    "        return colab_candidate\n",
    "    if Path(\"/content\").exists():\n",
    "        print(\"Repository not found; attempting to clone into /content/metalora ...\")\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"git\",\n",
    "                \"clone\",\n",
    "                \"https://github.com/doem97/metalora.git\",\n",
    "                str(colab_candidate),\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "        if (colab_candidate / marker).exists():\n",
    "            return colab_candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate {marker}. Set METALORA_ROOT to the repo path or clone it under /content/metalora.\"\n",
    "    )\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "if \"datasets\" in sys.modules:\n",
    "    del sys.modules[\"datasets\"]\n",
    "os.chdir(REPO_ROOT)\n",
    "\n",
    "import datasets\n",
    "from trainer import (\n",
    "    CLASS_MEAN_FNAME,\n",
    "    TEXT_FEAT_FNAME,\n",
    "    Trainer,\n",
    "    load_clip_to_cpu,\n",
    "    load_vit_to_cpu,\n",
    "    )\n",
    "from models import PeftModelFromCLIP, PeftModelFromViT, ZeroShotCLIP\n",
    "from models.satmae_vit import MAEViTAdapter\n",
    "from utils.config_omega import cfg as base_cfg\n",
    "from utils.evaluator import Evaluator\n",
    "from utils.logger import logger\n",
    "from utils.samplers import ClassAwareSampler, DownSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc52e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_config(repo_root, dataset_name, model_name, tuner_name=None, overrides=None):\n",
    "    config = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))\n",
    "    config_paths = [\n",
    "        repo_root / \"configs\" / \"data\" / f\"{dataset_name}.yaml\",\n",
    "        repo_root / \"configs\" / \"model\" / f\"{model_name}.yaml\",\n",
    "    ]\n",
    "    if tuner_name:\n",
    "        config_paths.append(repo_root / \"configs\" / \"tuner\" / f\"{tuner_name}.yaml\")\n",
    "    for path in config_paths:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(path)\n",
    "        config = OmegaConf.merge(config, OmegaConf.load(path))\n",
    "    if overrides:\n",
    "        config = OmegaConf.merge(config, OmegaConf.create(overrides))\n",
    "    return config\n",
    "\n",
    "\n",
    "def make_cifar_subset(dataset_cls, root, train, transform, class_indices, remap=True):\n",
    "    keep = sorted(class_indices)\n",
    "    dataset = dataset_cls(root, train=train, transform=transform)\n",
    "    targets = list(dataset.targets)\n",
    "    indices = [idx for idx, label in enumerate(targets) if label in keep]\n",
    "    if len(indices) == 0:\n",
    "        raise ValueError(\"No samples found for the provided classes.\")\n",
    "    index_array = np.array(indices, dtype=np.int64)\n",
    "    dataset.data = dataset.data[index_array]\n",
    "    selected_targets = [targets[idx] for idx in indices]\n",
    "    dataset.original_targets = selected_targets.copy()\n",
    "    subset_classnames = [dataset.classes[idx] for idx in keep]\n",
    "    if remap:\n",
    "        label_map = {orig: new_idx for new_idx, orig in enumerate(keep)}\n",
    "        remapped_targets = [label_map[label] for label in selected_targets]\n",
    "        dataset.targets = remapped_targets\n",
    "        dataset.labels = remapped_targets\n",
    "        dataset.classes = subset_classnames\n",
    "        dataset.classnames = subset_classnames\n",
    "        dataset.class_to_idx = {name: idx for idx, name in enumerate(subset_classnames)}\n",
    "        dataset.label_map = label_map\n",
    "        dataset.inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    else:\n",
    "        dataset.targets = selected_targets\n",
    "        dataset.labels = selected_targets\n",
    "        dataset.classnames = subset_classnames\n",
    "        dataset.label_map = None\n",
    "        dataset.inverse_label_map = None\n",
    "    if hasattr(dataset, \"get_cls_num_list\"):\n",
    "        dataset.cls_num_list = dataset.get_cls_num_list()\n",
    "        dataset.num_classes = len(dataset.cls_num_list)\n",
    "    else:\n",
    "        dataset.num_classes = len(subset_classnames)\n",
    "        dataset.cls_num_list = [dataset.targets.count(i) for i in range(dataset.num_classes)]\n",
    "    dataset.keep_classes = keep\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_ood_metrics(id_scores, ood_scores, tpr=0.95):\n",
    "    id_scores = np.asarray(id_scores, dtype=np.float32)\n",
    "    ood_scores = np.asarray(ood_scores, dtype=np.float32)\n",
    "    if id_scores.size == 0 or ood_scores.size == 0:\n",
    "        raise ValueError(\"Need non-empty ID and OOD score arrays.\")\n",
    "    labels = np.concatenate([np.ones_like(id_scores), np.zeros_like(ood_scores)])\n",
    "    scores = np.concatenate([id_scores, ood_scores])\n",
    "    threshold = np.percentile(id_scores, (1.0 - tpr) * 100.0)\n",
    "    metrics = {\n",
    "        \"auroc\": float(roc_auc_score(labels, scores)),\n",
    "        \"aupr\": float(average_precision_score(labels, scores)),\n",
    "        \"fpr@95tpr\": float(np.mean(ood_scores >= threshold)),\n",
    "        \"threshold@95tpr\": float(threshold),\n",
    "        \"id_mean\": float(id_scores.mean()),\n",
    "        \"id_std\": float(id_scores.std()),\n",
    "        \"ood_mean\": float(ood_scores.mean()),\n",
    "        \"ood_std\": float(ood_scores.std()),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassAwareOODTrainer(Trainer):\n",
    "    def __init__(self, cfg, device, id_classes, ood_classes=None, class_aware_k=4):\n",
    "        self.id_classes = sorted(set(id_classes))\n",
    "        self.ood_classes = sorted(set(ood_classes or []))\n",
    "        self.external_ood_name = getattr(cfg, \"ood_dataset\", None)\n",
    "        self.external_ood_name = (\n",
    "            self.external_ood_name.lower() if self.external_ood_name else None\n",
    "        )\n",
    "        overlap = set(self.id_classes) & set(self.ood_classes)\n",
    "        if overlap and not self.external_ood_name:\n",
    "            raise ValueError(\n",
    "                f\"In-distribution and OOD classes overlap: {sorted(overlap)}\"\n",
    "            )\n",
    "        self.class_aware_k = class_aware_k\n",
    "        super().__init__(cfg, device)\n",
    "        self.local_rank = 0\n",
    "        self.world_size = 1\n",
    "        self.ood_test_loader = None\n",
    "        root_hint = Path(cfg.root or os.environ.get(\"CIFAR100_ROOT\", \"./data\")).expanduser()\n",
    "        class_names = getattr(TorchvisionCIFAR100, \"classes\", None)\n",
    "        if class_names is None:\n",
    "            preview_dataset = TorchvisionCIFAR100(\n",
    "                root=str(root_hint), train=True, download=True\n",
    "            )\n",
    "            class_names = preview_dataset.classes\n",
    "        self.global_classnames = class_names\n",
    "        self.id_classnames = [class_names[idx] for idx in self.id_classes]\n",
    "        if self.external_ood_name:\n",
    "            self.ood_classnames = None\n",
    "        else:\n",
    "            self.ood_classnames = [class_names[idx] for idx in self.ood_classes]\n",
    "        self.last_ood_scores = None\n",
    "\n",
    "    def build_data_loader(self):\n",
    "        cfg = self.cfg\n",
    "        root = cfg.root\n",
    "        resolution = cfg.resolution\n",
    "\n",
    "        if cfg.backbone.startswith(\"CLIP\"):\n",
    "            mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "            std = [0.26862954, 0.26130258, 0.27577711]\n",
    "        else:\n",
    "            mean = [0.5, 0.5, 0.5]\n",
    "            std = [0.5, 0.5, 0.5]\n",
    "\n",
    "        transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(resolution),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    ")\n",
    "\n",
    "        transform_plain = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(resolution),\n",
    "                transforms.CenterCrop(resolution),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    ")\n",
    "\n",
    "        transform_test = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(resolution * 8 // 7),\n",
    "                transforms.CenterCrop(resolution),\n",
    "                transforms.Lambda(\n",
    "                    lambda crop: torch.stack([transforms.ToTensor()(crop)])\n",
    "                ),\n",
    "                transforms.Normalize(mean, std),\n",
    "            ]\n",
    ")\n",
    "\n",
    "        dataset_cls = getattr(datasets, cfg.dataset)\n",
    "        train_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, True, transform_train, self.id_classes, remap=True\n",
    "        )\n",
    "        train_init_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, True, transform_plain, self.id_classes, remap=True\n",
    "        )\n",
    "        train_test_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, True, transform_test, self.id_classes, remap=True\n",
    "        )\n",
    "        id_test_dataset = make_cifar_subset(\n",
    "            dataset_cls, root, False, transform_test, self.id_classes, remap=True\n",
    "        )\n",
    "\n",
    "        if self.external_ood_name:\n",
    "            ood_test_dataset = self._build_external_ood_dataset(transform_test)\n",
    "        else:\n",
    "            if not self.ood_classes:\n",
    "                raise ValueError(\n",
    "                    \"No OOD classes specified and external OOD dataset not provided.\"\n",
    "                )\n",
    "            ood_test_dataset = make_cifar_subset(\n",
    "                dataset_cls, root, False, transform_test, self.ood_classes, remap=False\n",
    "            )\n",
    "\n",
    "        self.num_classes = train_dataset.num_classes\n",
    "        self.cls_num_list = train_dataset.cls_num_list\n",
    "        self.classnames = train_dataset.classnames\n",
    "\n",
    "        freq = np.array(self.cls_num_list)\n",
    "        self.many_idxs = np.where(freq > 100)[0]\n",
    "        self.med_idxs = np.where((freq >= 20) & (freq <= 100))[0]\n",
    "        self.few_idxs = np.where(freq < 20)[0]\n",
    "\n",
    "        if cfg.init_head == \"1_shot\":\n",
    "            init_sampler = DownSampler(train_init_dataset, n_max=1)\n",
    "        elif cfg.init_head == \"10_shot\":\n",
    "            init_sampler = DownSampler(train_init_dataset, n_max=10)\n",
    "        elif cfg.init_head == \"100_shot\":\n",
    "            init_sampler = DownSampler(train_init_dataset, n_max=100)\n",
    "        else:\n",
    "            init_sampler = None\n",
    "\n",
    "        self.accum_step = cfg.accum_step or 1\n",
    "        self.eff_batch_size = cfg.batch_size\n",
    "        denom = self.accum_step * self.world_size\n",
    "        if self.eff_batch_size % denom != 0:\n",
    "            raise ValueError(\n",
    "                f\"batch_size ({cfg.batch_size}) must be divisible by accum_step ({self.accum_step}).\"\n",
    "            )\n",
    "        self.per_gpu_batch_size = self.eff_batch_size // denom\n",
    "\n",
    "        train_sampler = ClassAwareSampler(train_dataset, num_samples_cls=self.class_aware_k)\n",
    "\n",
    "        pin = self.device.type == \"cuda\"\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.per_gpu_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.train_init_loader = DataLoader(\n",
    "            train_init_dataset,\n",
    "            batch_size=min(64, len(train_init_dataset)),\n",
    "            sampler=init_sampler,\n",
    "            shuffle=init_sampler is None,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.train_test_loader = DataLoader(\n",
    "            train_test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.test_loader = DataLoader(\n",
    "            id_test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        self.ood_test_loader = DataLoader(\n",
    "            ood_test_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "\n",
    "        ood_desc = (\n",
    "            f\"OOD dataset ({self.external_ood_name.upper()}): {len(ood_test_dataset)} samples\"\n",
    "            if self.external_ood_name\n",
    "            else f\"OOD samples: {len(ood_test_dataset)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Train samples: {len(train_dataset)} | ID classes: {len(self.id_classes)} | {ood_desc}\"\n",
    "        )\n",
    "\n",
    "    def build_model(self):\n",
    "        cfg = self.cfg\n",
    "        classnames = self.classnames\n",
    "        num_classes = len(classnames)\n",
    "\n",
    "        if cfg.backbone.startswith(\"CLIP\"):\n",
    "            clip_model = load_clip_to_cpu(cfg.backbone, cfg.prec)\n",
    "            if cfg.zero_shot:\n",
    "                self.model = ZeroShotCLIP(clip_model)\n",
    "                self.model.to(self.device)\n",
    "                self.tuner = None\n",
    "                self.head = None\n",
    "                template = \"a photo of a {}.\"\n",
    "                prompts = self.get_tokenized_prompts(classnames, template)\n",
    "                self.model.init_text_features(prompts)\n",
    "                return\n",
    "            self.model = PeftModelFromCLIP(cfg, clip_model, num_classes)\n",
    "        elif cfg.backbone.startswith(\"IN21K-ViT\"):\n",
    "            vit_model = load_vit_to_cpu(cfg.backbone, cfg.prec)\n",
    "            self.model = PeftModelFromViT(cfg, vit_model, num_classes)\n",
    "        elif cfg.backbone.startswith(\"SatMAE-ViT\"):\n",
    "            vit_model = load_vit_to_cpu(cfg.backbone, cfg.prec)\n",
    "            self.model = PeftModelFromViT(cfg, vit_model, num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {cfg.backbone}\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.tuner = getattr(self.model, \"tuner\", None)\n",
    "        self.head = getattr(self.model, \"head\", None)\n",
    "\n",
    "        if cfg.init_head == \"text_feat\":\n",
    "            if not cfg.backbone.startswith(\"CLIP\"):\n",
    "                print(\"text_feat head init is only available for CLIP backbones.\")\n",
    "            else:\n",
    "                text_feat_fname = TEXT_FEAT_FNAME.get(cfg.backbone)\n",
    "                if text_feat_fname is None:\n",
    "                    raise ValueError(\n",
    "                        f\"No text feature file registered for {cfg.backbone}\"\n",
    "                    )\n",
    "                if cfg.head_init_folder is None:\n",
    "                    raise ValueError(\n",
    "                        \"head_init_folder must be set for text feature initialization.\"\n",
    "                    )\n",
    "                text_feat_path = os.path.join(cfg.head_init_folder, text_feat_fname)\n",
    "                self.init_head_text_feat(text_feat_path)\n",
    "        elif cfg.init_head in [\"class_mean\", \"1_shot\", \"10_shot\", \"100_shot\"]:\n",
    "            class_mean_fname = CLASS_MEAN_FNAME.get(cfg.backbone)\n",
    "            if class_mean_fname is None:\n",
    "                raise ValueError(\n",
    "                    f\"No class mean file registered for {cfg.backbone}\"\n",
    "                )\n",
    "            if cfg.head_init_folder is None:\n",
    "                raise ValueError(\n",
    "                    \"head_init_folder must be set for class mean initialization.\"\n",
    "                )\n",
    "            class_mean_path = os.path.join(cfg.head_init_folder, class_mean_fname)\n",
    "            self.init_head_class_mean(class_mean_path)\n",
    "        elif cfg.init_head == \"linear_probe\":\n",
    "            self.init_head_linear_probe()\n",
    "\n",
    "        if not (cfg.zero_shot or cfg.test_train or cfg.test_only):\n",
    "            self.build_optimizer()\n",
    "            self.build_criterion()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    def _build_external_ood_dataset(self, transform):\n",
    "        dataset_name = self.external_ood_name\n",
    "        if dataset_name == \"svhn\":\n",
    "            root = getattr(self.cfg, \"ood_root\", None)\n",
    "            if root is None:\n",
    "                root = Path(self.cfg.root).expanduser() / \"svhn\"\n",
    "            root = Path(root).expanduser()\n",
    "            split = getattr(self.cfg, \"ood_split\", \"test\")\n",
    "            return TorchvisionSVHN(\n",
    "                root=str(root),\n",
    "                split=split,\n",
    "                download=True,\n",
    "                transform=transform,\n",
    "            )\n",
    "        raise ValueError(\n",
    "            f\"Unsupported external OOD dataset: {self.external_ood_name}\"\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_ood(self):\n",
    "        if self.ood_test_loader is None:\n",
    "            raise RuntimeError(\"OOD loader not initialized.\")\n",
    "        self.model.eval()\n",
    "        if self.tuner is not None:\n",
    "            self.tuner.eval()\n",
    "        if self.head is not None:\n",
    "            self.head.eval()\n",
    "\n",
    "        amp_enabled = self.cfg.prec == \"amp\" and self.device.type == \"cuda\"\n",
    "\n",
    "        def collect_scores(loader):\n",
    "            scores = []\n",
    "            for images, _ in loader:\n",
    "                images = images.to(self.device)\n",
    "                batch_size, ncrops, c, h, w = images.size()\n",
    "                images = images.view(batch_size * ncrops, c, h, w)\n",
    "                with autocast(enabled=amp_enabled):\n",
    "                    logits = self.model(images)\n",
    "                logits = logits.view(batch_size, ncrops, -1).mean(dim=1)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                scores.extend(probs.max(dim=1)[0].cpu().numpy())\n",
    "            return scores\n",
    "\n",
    "        id_scores = collect_scores(self.test_loader)\n",
    "        ood_scores = collect_scores(self.ood_test_loader)\n",
    "        metrics = compute_ood_metrics(id_scores, ood_scores)\n",
    "        self.last_ood_scores = {\"id\": id_scores, \"ood\": ood_scores, \"metrics\": metrics}\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34450219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:02<00:00, 61.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID classes: 80 (first 10): ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle']\n",
      "OOD classes: 20 (first 5): ['squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset': 'CIFAR100', 'root': '/content/metalora/data', 'imb_factor': None, 'head_init_folder': '/content/metalora/output/notebooks/cifar100_class_aware_ood', 'backbone': 'CLIP-ViT-B/16', 'resolution': 224, 'output_dir': '/content/metalora/output/notebooks/cifar100_class_aware_ood', 'print_freq': 20, 'seed': 0, 'deterministic': True, 'num_workers': 2, 'prec': 'fp32', 'num_epochs': 5, 'batch_size': 64, 'accum_step': 1, 'lr': 0.01, 'scheduler': 'CosineAnnealingLR', 'weight_decay': 0.0005, 'momentum': 0.9, 'loss_type': 'CE', 'classifier': 'CosineClassifier', 'scale': 25, 'fine_tuning': False, 'head_only': True, 'full_tuning': False, 'bias_tuning': False, 'ln_tuning': False, 'bn_tuning': False, 'vpt_shallow': False, 'vpt_deep': False, 'adapter': False, 'adaptformer': False, 'lora': False, 'lora_mlp': False, 'scale_alpha': 1, 'ssf_attn': False, 'ssf_mlp': False, 'ssf_ln': False, 'mask': False, 'partial': None, 'vpt_len': None, 'adapter_dim': None, 'adaptformer_scale': 'learnable', 'mask_ratio': None, 'mask_seed': None, 'init_head': 'text_feat', 'prompt': 'default', 'tte': False, 'expand': 24, 'tte_mode': 'fivecrop', 'randaug_times': 1, 'zero_shot': False, 'test_only': False, 'test_train': False, 'model_dir': None, 'use_flora': False, 'flora': {'arch': {'modules': ['q', 'v', 'k', 'out', 'mlp1', 'mlp2'], 'layers': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'rank': 4, 'alpha': None}, 'optimizer': {'default_lr': 0.02, 'lr_config': {'layers': {}, 'modules': {}, 'specific': {}}}}, 'use_meta': False, 'meta_data_ratio': 0.1, 'meta_lr': 0.001, 'meta_update_freq': 1, 'meta_inner_steps': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_CLASSES = list(range(80))\n",
    "OOD_CLASSES = list(range(80, 100))\n",
    "\n",
    "default_data_root = os.environ.get(\"CIFAR100_ROOT\") or str((REPO_ROOT / \"data\").resolve())\n",
    "class_names = getattr(TorchvisionCIFAR100, \"classes\", None)\n",
    "if class_names is None:\n",
    "    preview_dataset = TorchvisionCIFAR100(root=default_data_root, train=True, download=True)\n",
    "    class_names = preview_dataset.classes\n",
    "\n",
    "id_names_preview = [class_names[idx] for idx in ID_CLASSES[:10]]\n",
    "ood_names_preview = [class_names[idx] for idx in OOD_CLASSES[:5]]\n",
    "print(f\"ID classes: {len(ID_CLASSES)} (first 10): {id_names_preview}\")\n",
    "print(f\"OOD classes: {len(OOD_CLASSES)} (first 5): {ood_names_preview}\")\n",
    "\n",
    "cfg = load_experiment_config(\n",
    "    REPO_ROOT,\n",
    "    dataset_name=\"cifar100\",\n",
    "    model_name=\"clip_vit_b16\",\n",
    "    tuner_name=None,\n",
    ")\n",
    "\n",
    "cfg.use_meta = False\n",
    "cfg.output_dir = str(REPO_ROOT / \"output\" / \"notebooks\" / \"cifar100_class_aware_ood\")\n",
    "cfg.root = default_data_root\n",
    "cfg.num_epochs = 5\n",
    "cfg.batch_size = 64\n",
    "cfg.accum_step = 1\n",
    "cfg.loss_type = \"CE\"\n",
    "cfg.head_only = True\n",
    "cfg.init_head = \"text_feat\"\n",
    "cfg.tte = False\n",
    "cfg.lr = 0.01\n",
    "cfg.print_freq = 20\n",
    "cfg.seed = 0\n",
    "cfg.deterministic = True\n",
    "cfg.num_workers = min(4, os.cpu_count() or 4)\n",
    "cfg.prec = \"amp\" if device.type == \"cuda\" else \"fp32\"\n",
    "cfg.head_init_folder = cfg.output_dir\n",
    "\n",
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "if cfg.seed is not None:\n",
    "    random.seed(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(cfg.seed)\n",
    "        torch.cuda.manual_seed_all(cfg.seed)\n",
    "\n",
    "if cfg.deterministic and torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "logger.init(cfg.output_dir)\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1c8d599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mTrain samples: 40000 | ID classes: 80 | OOD samples: 2000\u001b[0m\n",
      "\u001b[37mFLoRA not used.\u001b[0m\n",
      "\u001b[37mViT_Tuner initialization complete\u001b[0m\n",
      "\u001b[37mFLoRA not used.\u001b[0m\n",
      "\u001b[37mViT_Tuner initialization complete\u001b[0m\n",
      "\u001b[37mLoading text features from /content/metalora/output/notebooks/cifar100_class_aware_ood/txtfeat_clip_vit_b16.pth\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Optimizer                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mHead-only mode: Only training the classifier head\u001b[0m\n",
      "\u001b[37mTotal params: 149682176\u001b[0m\n",
      "\u001b[37mTuner params: 0\u001b[0m\n",
      "\u001b[37mHead params: 61440\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:453: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if cfg.prec == \"amp\" else None\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Criterion                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mUsing loss type: CE\u001b[0m\n",
      "\u001b[37mStandard Cross Entropy Loss\u001b[0m\n",
      "\u001b[37m\n",
      "Class Distribution Summary:\u001b[0m\n",
      "\u001b[37mTotal samples: 40000\u001b[0m\n",
      "\u001b[37mLoading text features from /content/metalora/output/notebooks/cifar100_class_aware_ood/txtfeat_clip_vit_b16.pth\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Optimizer                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mHead-only mode: Only training the classifier head\u001b[0m\n",
      "\u001b[37mTotal params: 149682176\u001b[0m\n",
      "\u001b[37mTuner params: 0\u001b[0m\n",
      "\u001b[37mHead params: 61440\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:453: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if cfg.prec == \"amp\" else None\u001b[0m\n",
      "\u001b[37m\n",
      "================================================================================\n",
      "                               Building Criterion                               \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mUsing loss type: CE\u001b[0m\n",
      "\u001b[37mStandard Cross Entropy Loss\u001b[0m\n",
      "\u001b[37m\n",
      "Class Distribution Summary:\u001b[0m\n",
      "\u001b[37mTotal samples: 40000\u001b[0m\n",
      "\u001b[37mMax samples per class: 500\u001b[0m\n",
      "\u001b[37mMin samples per class: 500\u001b[0m\n",
      "\u001b[37mImbalance ratio: 1.00\u001b[0m\n",
      "\u001b[37mInitialize tensorboard (log_dir=/content/metalora/output/notebooks/cifar100_class_aware_ood/tensorboard)\u001b[0m\n",
      "\u001b[37mMax samples per class: 500\u001b[0m\n",
      "\u001b[37mMin samples per class: 500\u001b[0m\n",
      "\u001b[37mImbalance ratio: 1.00\u001b[0m\n",
      "\u001b[37mInitialize tensorboard (log_dir=/content/metalora/output/notebooks/cifar100_class_aware_ood/tensorboard)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = ClassAwareOODTrainer(cfg, device, ID_CLASSES, OOD_CLASSES, class_aware_k=4)\n",
    "trainer.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa29caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\n",
      "================================================================================\n",
      "                                 Training model                                 \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:769: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\u001b[0m\n",
      "\u001b[31m/content/metalora/trainer.py:769: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [20/625] time 0.255 (0.277) data 0.000 (0.022) loss 3.4210 (3.4301) acc 29.6875 (32.1360) (mean 31.8249 many 31.8249 med nan few nan) lr 1.0000e-02 elapsed 0:00:05 eta 0:14:18\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [20/625] time 0.255 (0.277) data 0.000 (0.022) loss 3.4210 (3.4301) acc 29.6875 (32.1360) (mean 31.8249 many 31.8249 med nan few nan) lr 1.0000e-02 elapsed 0:00:05 eta 0:14:18\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [40/625] time 0.252 (0.265) data 0.000 (0.012) loss 3.2935 (3.1031) acc 31.2500 (36.7725) (mean 36.8778 many 36.8778 med nan few nan) lr 1.0000e-02 elapsed 0:00:10 eta 0:13:36\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [40/625] time 0.252 (0.265) data 0.000 (0.012) loss 3.2935 (3.1031) acc 31.2500 (36.7725) (mean 36.8778 many 36.8778 med nan few nan) lr 1.0000e-02 elapsed 0:00:10 eta 0:13:36\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [60/625] time 0.251 (0.260) data 0.000 (0.008) loss 2.5872 (2.8351) acc 45.3125 (39.7871) (mean 39.8836 many 39.8836 med nan few nan) lr 1.0000e-02 elapsed 0:00:15 eta 0:13:17\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [60/625] time 0.251 (0.260) data 0.000 (0.008) loss 2.5872 (2.8351) acc 45.3125 (39.7871) (mean 39.8836 many 39.8836 med nan few nan) lr 1.0000e-02 elapsed 0:00:15 eta 0:13:17\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [80/625] time 0.251 (0.259) data 0.000 (0.006) loss 2.7756 (2.6715) acc 40.6250 (41.9341) (mean 42.1052 many 42.1052 med nan few nan) lr 1.0000e-02 elapsed 0:00:20 eta 0:13:07\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [80/625] time 0.251 (0.259) data 0.000 (0.006) loss 2.7756 (2.6715) acc 40.6250 (41.9341) (mean 42.1052 many 42.1052 med nan few nan) lr 1.0000e-02 elapsed 0:00:20 eta 0:13:07\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [100/625] time 0.255 (0.257) data 0.001 (0.005) loss 2.8716 (2.5629) acc 29.6875 (42.1940) (mean 42.8348 many 42.8348 med nan few nan) lr 1.0000e-02 elapsed 0:00:25 eta 0:12:58\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [100/625] time 0.255 (0.257) data 0.001 (0.005) loss 2.8716 (2.5629) acc 29.6875 (42.1940) (mean 42.8348 many 42.8348 med nan few nan) lr 1.0000e-02 elapsed 0:00:25 eta 0:12:58\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [120/625] time 0.260 (0.257) data 0.003 (0.004) loss 2.5465 (2.4985) acc 31.2500 (41.5887) (mean 42.4795 many 42.4795 med nan few nan) lr 1.0000e-02 elapsed 0:00:30 eta 0:12:51\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [120/625] time 0.260 (0.257) data 0.003 (0.004) loss 2.5465 (2.4985) acc 31.2500 (41.5887) (mean 42.4795 many 42.4795 med nan few nan) lr 1.0000e-02 elapsed 0:00:30 eta 0:12:51\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [140/625] time 0.251 (0.257) data 0.000 (0.004) loss 2.6631 (2.4163) acc 31.2500 (44.3902) (mean 45.0068 many 45.0068 med nan few nan) lr 1.0000e-02 elapsed 0:00:35 eta 0:12:46\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [140/625] time 0.251 (0.257) data 0.000 (0.004) loss 2.6631 (2.4163) acc 31.2500 (44.3902) (mean 45.0068 many 45.0068 med nan few nan) lr 1.0000e-02 elapsed 0:00:35 eta 0:12:46\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [160/625] time 0.257 (0.257) data 0.000 (0.003) loss 2.3234 (2.3411) acc 43.7500 (46.1454) (mean 46.5791 many 46.5791 med nan few nan) lr 1.0000e-02 elapsed 0:00:41 eta 0:12:40\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [160/625] time 0.257 (0.257) data 0.000 (0.003) loss 2.3234 (2.3411) acc 43.7500 (46.1454) (mean 46.5791 many 46.5791 med nan few nan) lr 1.0000e-02 elapsed 0:00:41 eta 0:12:40\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [180/625] time 0.252 (0.257) data 0.000 (0.003) loss 2.0979 (2.3205) acc 42.1875 (45.6081) (mean 46.1748 many 46.1748 med nan few nan) lr 1.0000e-02 elapsed 0:00:46 eta 0:12:36\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [180/625] time 0.252 (0.257) data 0.000 (0.003) loss 2.0979 (2.3205) acc 42.1875 (45.6081) (mean 46.1748 many 46.1748 med nan few nan) lr 1.0000e-02 elapsed 0:00:46 eta 0:12:36\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [200/625] time 0.258 (0.257) data 0.000 (0.003) loss 2.1250 (2.2941) acc 54.6875 (45.8895) (mean 45.7058 many 45.7058 med nan few nan) lr 1.0000e-02 elapsed 0:00:51 eta 0:12:31\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [200/625] time 0.258 (0.257) data 0.000 (0.003) loss 2.1250 (2.2941) acc 54.6875 (45.8895) (mean 45.7058 many 45.7058 med nan few nan) lr 1.0000e-02 elapsed 0:00:51 eta 0:12:31\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [220/625] time 0.254 (0.257) data 0.001 (0.003) loss 2.3554 (2.1803) acc 42.1875 (48.2455) (mean 48.5594 many 48.5594 med nan few nan) lr 1.0000e-02 elapsed 0:00:56 eta 0:12:26\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [220/625] time 0.254 (0.257) data 0.001 (0.003) loss 2.3554 (2.1803) acc 42.1875 (48.2455) (mean 48.5594 many 48.5594 med nan few nan) lr 1.0000e-02 elapsed 0:00:56 eta 0:12:26\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [240/625] time 0.256 (0.257) data 0.000 (0.002) loss 1.9846 (2.1845) acc 51.5625 (48.2949) (mean 48.7239 many 48.7239 med nan few nan) lr 1.0000e-02 elapsed 0:01:01 eta 0:12:21\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [240/625] time 0.256 (0.257) data 0.000 (0.002) loss 1.9846 (2.1845) acc 51.5625 (48.2949) (mean 48.7239 many 48.7239 med nan few nan) lr 1.0000e-02 elapsed 0:01:01 eta 0:12:21\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [260/625] time 0.257 (0.257) data 0.000 (0.002) loss 2.2064 (2.1700) acc 45.3125 (46.9579) (mean 47.6315 many 47.6315 med nan few nan) lr 1.0000e-02 elapsed 0:01:06 eta 0:12:16\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [260/625] time 0.257 (0.257) data 0.000 (0.002) loss 2.2064 (2.1700) acc 45.3125 (46.9579) (mean 47.6315 many 47.6315 med nan few nan) lr 1.0000e-02 elapsed 0:01:06 eta 0:12:16\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [280/625] time 0.259 (0.257) data 0.000 (0.002) loss 2.4014 (2.1513) acc 40.6250 (48.3279) (mean 48.8232 many 48.8232 med nan few nan) lr 1.0000e-02 elapsed 0:01:11 eta 0:12:11\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [280/625] time 0.259 (0.257) data 0.000 (0.002) loss 2.4014 (2.1513) acc 40.6250 (48.3279) (mean 48.8232 many 48.8232 med nan few nan) lr 1.0000e-02 elapsed 0:01:11 eta 0:12:11\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [300/625] time 0.257 (0.257) data 0.000 (0.002) loss 2.2739 (2.2029) acc 46.8750 (47.5483) (mean 47.5936 many 47.5936 med nan few nan) lr 1.0000e-02 elapsed 0:01:17 eta 0:12:06\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [300/625] time 0.257 (0.257) data 0.000 (0.002) loss 2.2739 (2.2029) acc 46.8750 (47.5483) (mean 47.5936 many 47.5936 med nan few nan) lr 1.0000e-02 elapsed 0:01:17 eta 0:12:06\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [320/625] time 0.260 (0.257) data 0.001 (0.002) loss 2.3908 (2.1040) acc 42.1875 (49.6043) (mean 49.9182 many 49.9182 med nan few nan) lr 1.0000e-02 elapsed 0:01:22 eta 0:12:01\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [320/625] time 0.260 (0.257) data 0.001 (0.002) loss 2.3908 (2.1040) acc 42.1875 (49.6043) (mean 49.9182 many 49.9182 med nan few nan) lr 1.0000e-02 elapsed 0:01:22 eta 0:12:01\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [340/625] time 0.261 (0.257) data 0.001 (0.002) loss 1.8532 (2.0522) acc 50.0000 (50.1675) (mean 50.3677 many 50.3677 med nan few nan) lr 1.0000e-02 elapsed 0:01:27 eta 0:11:56\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [340/625] time 0.261 (0.257) data 0.001 (0.002) loss 1.8532 (2.0522) acc 50.0000 (50.1675) (mean 50.3677 many 50.3677 med nan few nan) lr 1.0000e-02 elapsed 0:01:27 eta 0:11:56\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [360/625] time 0.263 (0.258) data 0.000 (0.002) loss 2.3077 (2.0839) acc 46.8750 (52.0881) (mean 51.9641 many 51.9641 med nan few nan) lr 1.0000e-02 elapsed 0:01:32 eta 0:11:52\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [360/625] time 0.263 (0.258) data 0.000 (0.002) loss 2.3077 (2.0839) acc 46.8750 (52.0881) (mean 51.9641 many 51.9641 med nan few nan) lr 1.0000e-02 elapsed 0:01:32 eta 0:11:52\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [380/625] time 0.258 (0.258) data 0.000 (0.002) loss 1.8854 (2.1122) acc 59.3750 (48.4531) (mean 48.3605 many 48.3605 med nan few nan) lr 1.0000e-02 elapsed 0:01:37 eta 0:11:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [380/625] time 0.258 (0.258) data 0.000 (0.002) loss 1.8854 (2.1122) acc 59.3750 (48.4531) (mean 48.3605 many 48.3605 med nan few nan) lr 1.0000e-02 elapsed 0:01:37 eta 0:11:47\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [400/625] time 0.263 (0.258) data 0.000 (0.002) loss 2.2184 (2.0210) acc 46.8750 (52.3621) (mean 52.2024 many 52.2024 med nan few nan) lr 1.0000e-02 elapsed 0:01:43 eta 0:11:42\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [400/625] time 0.263 (0.258) data 0.000 (0.002) loss 2.2184 (2.0210) acc 46.8750 (52.3621) (mean 52.2024 many 52.2024 med nan few nan) lr 1.0000e-02 elapsed 0:01:43 eta 0:11:42\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [420/625] time 0.261 (0.258) data 0.000 (0.002) loss 1.9730 (2.0154) acc 50.0000 (51.7321) (mean 52.0297 many 52.0297 med nan few nan) lr 1.0000e-02 elapsed 0:01:48 eta 0:11:38\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [420/625] time 0.261 (0.258) data 0.000 (0.002) loss 1.9730 (2.0154) acc 50.0000 (51.7321) (mean 52.0297 many 52.0297 med nan few nan) lr 1.0000e-02 elapsed 0:01:48 eta 0:11:38\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [440/625] time 0.268 (0.258) data 0.000 (0.002) loss 1.9505 (2.0097) acc 53.1250 (51.8779) (mean 51.7422 many 51.7422 med nan few nan) lr 1.0000e-02 elapsed 0:01:53 eta 0:11:33\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [440/625] time 0.268 (0.258) data 0.000 (0.002) loss 1.9505 (2.0097) acc 53.1250 (51.8779) (mean 51.7422 many 51.7422 med nan few nan) lr 1.0000e-02 elapsed 0:01:53 eta 0:11:33\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [460/625] time 0.267 (0.259) data 0.001 (0.002) loss 2.4829 (2.0372) acc 32.8125 (48.6782) (mean 49.4503 many 49.4503 med nan few nan) lr 1.0000e-02 elapsed 0:01:58 eta 0:11:29\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [460/625] time 0.267 (0.259) data 0.001 (0.002) loss 2.4829 (2.0372) acc 32.8125 (48.6782) (mean 49.4503 many 49.4503 med nan few nan) lr 1.0000e-02 elapsed 0:01:58 eta 0:11:29\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [480/625] time 0.265 (0.259) data 0.000 (0.002) loss 2.5054 (2.0114) acc 32.8125 (51.1290) (mean 51.2637 many 51.2637 med nan few nan) lr 1.0000e-02 elapsed 0:02:04 eta 0:11:24\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [480/625] time 0.265 (0.259) data 0.000 (0.002) loss 2.5054 (2.0114) acc 32.8125 (51.1290) (mean 51.2637 many 51.2637 med nan few nan) lr 1.0000e-02 elapsed 0:02:04 eta 0:11:24\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [500/625] time 0.263 (0.259) data 0.000 (0.001) loss 2.0344 (2.0115) acc 48.4375 (49.3952) (mean 49.7427 many 49.7427 med nan few nan) lr 1.0000e-02 elapsed 0:02:09 eta 0:11:19\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [500/625] time 0.263 (0.259) data 0.000 (0.001) loss 2.0344 (2.0115) acc 48.4375 (49.3952) (mean 49.7427 many 49.7427 med nan few nan) lr 1.0000e-02 elapsed 0:02:09 eta 0:11:19\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [520/625] time 0.264 (0.259) data 0.000 (0.001) loss 2.1750 (2.0686) acc 46.8750 (48.1854) (mean 48.6735 many 48.6735 med nan few nan) lr 1.0000e-02 elapsed 0:02:14 eta 0:11:14\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [520/625] time 0.264 (0.259) data 0.000 (0.001) loss 2.1750 (2.0686) acc 46.8750 (48.1854) (mean 48.6735 many 48.6735 med nan few nan) lr 1.0000e-02 elapsed 0:02:14 eta 0:11:14\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [540/625] time 0.263 (0.259) data 0.001 (0.001) loss 1.8251 (1.9349) acc 53.1250 (51.8582) (mean 51.3703 many 51.3703 med nan few nan) lr 1.0000e-02 elapsed 0:02:20 eta 0:11:10\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [540/625] time 0.263 (0.259) data 0.001 (0.001) loss 1.8251 (1.9349) acc 53.1250 (51.8582) (mean 51.3703 many 51.3703 med nan few nan) lr 1.0000e-02 elapsed 0:02:20 eta 0:11:10\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [560/625] time 0.263 (0.259) data 0.000 (0.001) loss 2.1728 (2.0612) acc 50.0000 (49.9348) (mean 50.3893 many 50.3893 med nan few nan) lr 1.0000e-02 elapsed 0:02:25 eta 0:11:05\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [560/625] time 0.263 (0.259) data 0.000 (0.001) loss 2.1728 (2.0612) acc 50.0000 (49.9348) (mean 50.3893 many 50.3893 med nan few nan) lr 1.0000e-02 elapsed 0:02:25 eta 0:11:05\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [580/625] time 0.265 (0.260) data 0.000 (0.001) loss 1.6667 (1.9430) acc 57.8125 (51.2920) (mean 51.4549 many 51.4549 med nan few nan) lr 1.0000e-02 elapsed 0:02:30 eta 0:11:00\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [580/625] time 0.265 (0.260) data 0.000 (0.001) loss 1.6667 (1.9430) acc 57.8125 (51.2920) (mean 51.4549 many 51.4549 med nan few nan) lr 1.0000e-02 elapsed 0:02:30 eta 0:11:00\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [600/625] time 0.279 (0.260) data 0.000 (0.001) loss 2.0925 (1.8922) acc 42.1875 (52.7554) (mean 52.8362 many 52.8362 med nan few nan) lr 1.0000e-02 elapsed 0:02:35 eta 0:10:55\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [600/625] time 0.279 (0.260) data 0.000 (0.001) loss 2.0925 (1.8922) acc 42.1875 (52.7554) (mean 52.8362 many 52.8362 med nan few nan) lr 1.0000e-02 elapsed 0:02:35 eta 0:10:55\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [620/625] time 0.265 (0.260) data 0.000 (0.001) loss 1.8914 (1.9918) acc 54.6875 (51.5032) (mean 52.0680 many 52.0680 med nan few nan) lr 1.0000e-02 elapsed 0:02:41 eta 0:10:50\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [620/625] time 0.265 (0.260) data 0.000 (0.001) loss 1.8914 (1.9918) acc 54.6875 (51.5032) (mean 52.0680 many 52.0680 med nan few nan) lr 1.0000e-02 elapsed 0:02:41 eta 0:10:50\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [625/625] time 0.262 (0.260) data 0.000 (0.001) loss 2.2516 (2.0545) acc 42.1875 (49.2088) (mean 49.8353 many 49.8353 med nan few nan) lr 1.0000e-02 elapsed 0:02:42 eta 0:10:49\u001b[0m\n",
      "\u001b[37mepoch [1/5] batch [625/625] time 0.262 (0.260) data 0.000 (0.001) loss 2.2516 (2.0545) acc 42.1875 (49.2088) (mean 49.8353 many 49.8353 med nan few nan) lr 1.0000e-02 elapsed 0:02:42 eta 0:10:49\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [20/625] time 0.266 (0.261) data 0.000 (0.002) loss 1.9455 (1.9839) acc 51.5625 (52.4356) (mean 52.5352 many 52.5352 med nan few nan) lr 9.0451e-03 elapsed 0:02:48 eta 0:10:46\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [20/625] time 0.266 (0.261) data 0.000 (0.002) loss 1.9455 (1.9839) acc 51.5625 (52.4356) (mean 52.5352 many 52.5352 med nan few nan) lr 9.0451e-03 elapsed 0:02:48 eta 0:10:46\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [40/625] time 0.264 (0.261) data 0.000 (0.002) loss 2.1623 (1.9123) acc 46.8750 (53.0219) (mean 53.4743 many 53.4743 med nan few nan) lr 9.0451e-03 elapsed 0:02:53 eta 0:10:41\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [40/625] time 0.264 (0.261) data 0.000 (0.002) loss 2.1623 (1.9123) acc 46.8750 (53.0219) (mean 53.4743 many 53.4743 med nan few nan) lr 9.0451e-03 elapsed 0:02:53 eta 0:10:41\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [60/625] time 0.263 (0.261) data 0.000 (0.002) loss 1.9923 (1.9244) acc 56.2500 (53.8424) (mean 53.4953 many 53.4953 med nan few nan) lr 9.0451e-03 elapsed 0:02:58 eta 0:10:36\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [60/625] time 0.263 (0.261) data 0.000 (0.002) loss 1.9923 (1.9244) acc 56.2500 (53.8424) (mean 53.4953 many 53.4953 med nan few nan) lr 9.0451e-03 elapsed 0:02:58 eta 0:10:36\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [80/625] time 0.263 (0.261) data 0.000 (0.002) loss 1.7555 (1.8845) acc 57.8125 (54.8484) (mean 54.8547 many 54.8547 med nan few nan) lr 9.0451e-03 elapsed 0:03:04 eta 0:10:31\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [80/625] time 0.263 (0.261) data 0.000 (0.002) loss 1.7555 (1.8845) acc 57.8125 (54.8484) (mean 54.8547 many 54.8547 med nan few nan) lr 9.0451e-03 elapsed 0:03:04 eta 0:10:31\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [100/625] time 0.265 (0.261) data 0.000 (0.002) loss 1.8511 (1.9761) acc 56.2500 (51.6633) (mean 51.7067 many 51.7067 med nan few nan) lr 9.0451e-03 elapsed 0:03:09 eta 0:10:26\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [100/625] time 0.265 (0.261) data 0.000 (0.002) loss 1.8511 (1.9761) acc 56.2500 (51.6633) (mean 51.7067 many 51.7067 med nan few nan) lr 9.0451e-03 elapsed 0:03:09 eta 0:10:26\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [120/625] time 0.276 (0.261) data 0.002 (0.002) loss 2.4048 (2.0506) acc 34.3750 (49.4777) (mean 50.5340 many 50.5340 med nan few nan) lr 9.0451e-03 elapsed 0:03:14 eta 0:10:21\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [120/625] time 0.276 (0.261) data 0.002 (0.002) loss 2.4048 (2.0506) acc 34.3750 (49.4777) (mean 50.5340 many 50.5340 med nan few nan) lr 9.0451e-03 elapsed 0:03:14 eta 0:10:21\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [140/625] time 0.260 (0.261) data 0.000 (0.002) loss 2.0198 (1.9285) acc 46.8750 (52.8198) (mean 53.0599 many 53.0599 med nan few nan) lr 9.0451e-03 elapsed 0:03:20 eta 0:10:16\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [140/625] time 0.260 (0.261) data 0.000 (0.002) loss 2.0198 (1.9285) acc 46.8750 (52.8198) (mean 53.0599 many 53.0599 med nan few nan) lr 9.0451e-03 elapsed 0:03:20 eta 0:10:16\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [160/625] time 0.271 (0.261) data 0.000 (0.002) loss 2.1372 (1.8774) acc 43.7500 (52.5003) (mean 52.7816 many 52.7816 med nan few nan) lr 9.0451e-03 elapsed 0:03:25 eta 0:10:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [160/625] time 0.271 (0.261) data 0.000 (0.002) loss 2.1372 (1.8774) acc 43.7500 (52.5003) (mean 52.7816 many 52.7816 med nan few nan) lr 9.0451e-03 elapsed 0:03:25 eta 0:10:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [180/625] time 0.262 (0.262) data 0.000 (0.002) loss 1.3906 (1.8171) acc 67.1875 (54.1710) (mean 54.0189 many 54.0189 med nan few nan) lr 9.0451e-03 elapsed 0:03:30 eta 0:10:06\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [180/625] time 0.262 (0.262) data 0.000 (0.002) loss 1.3906 (1.8171) acc 67.1875 (54.1710) (mean 54.0189 many 54.0189 med nan few nan) lr 9.0451e-03 elapsed 0:03:30 eta 0:10:06\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [200/625] time 0.264 (0.262) data 0.000 (0.002) loss 1.8139 (1.9006) acc 59.3750 (52.4683) (mean 52.4153 many 52.4153 med nan few nan) lr 9.0451e-03 elapsed 0:03:36 eta 0:10:01\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [200/625] time 0.264 (0.262) data 0.000 (0.002) loss 1.8139 (1.9006) acc 59.3750 (52.4683) (mean 52.4153 many 52.4153 med nan few nan) lr 9.0451e-03 elapsed 0:03:36 eta 0:10:01\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [220/625] time 0.269 (0.262) data 0.000 (0.002) loss 1.7136 (1.9106) acc 62.5000 (52.9460) (mean 52.6894 many 52.6894 med nan few nan) lr 9.0451e-03 elapsed 0:03:41 eta 0:09:57\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [220/625] time 0.269 (0.262) data 0.000 (0.002) loss 1.7136 (1.9106) acc 62.5000 (52.9460) (mean 52.6894 many 52.6894 med nan few nan) lr 9.0451e-03 elapsed 0:03:41 eta 0:09:57\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [240/625] time 0.267 (0.262) data 0.000 (0.002) loss 2.3425 (2.0026) acc 45.3125 (51.7039) (mean 52.1380 many 52.1380 med nan few nan) lr 9.0451e-03 elapsed 0:03:46 eta 0:09:52\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [240/625] time 0.267 (0.262) data 0.000 (0.002) loss 2.3425 (2.0026) acc 45.3125 (51.7039) (mean 52.1380 many 52.1380 med nan few nan) lr 9.0451e-03 elapsed 0:03:46 eta 0:09:52\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [260/625] time 0.274 (0.262) data 0.004 (0.002) loss 2.0441 (1.8996) acc 54.6875 (54.5857) (mean 54.5577 many 54.5577 med nan few nan) lr 9.0451e-03 elapsed 0:03:52 eta 0:09:47\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [260/625] time 0.274 (0.262) data 0.004 (0.002) loss 2.0441 (1.8996) acc 54.6875 (54.5857) (mean 54.5577 many 54.5577 med nan few nan) lr 9.0451e-03 elapsed 0:03:52 eta 0:09:47\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [280/625] time 0.263 (0.262) data 0.000 (0.002) loss 2.0449 (1.8897) acc 53.1250 (52.3104) (mean 53.0908 many 53.0908 med nan few nan) lr 9.0451e-03 elapsed 0:03:57 eta 0:09:41\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [280/625] time 0.263 (0.262) data 0.000 (0.002) loss 2.0449 (1.8897) acc 53.1250 (52.3104) (mean 53.0908 many 53.0908 med nan few nan) lr 9.0451e-03 elapsed 0:03:57 eta 0:09:41\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [300/625] time 0.263 (0.262) data 0.000 (0.002) loss 2.0787 (1.9801) acc 45.3125 (49.6002) (mean 50.5617 many 50.5617 med nan few nan) lr 9.0451e-03 elapsed 0:04:02 eta 0:09:36\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [300/625] time 0.263 (0.262) data 0.000 (0.002) loss 2.0787 (1.9801) acc 45.3125 (49.6002) (mean 50.5617 many 50.5617 med nan few nan) lr 9.0451e-03 elapsed 0:04:02 eta 0:09:36\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [320/625] time 0.265 (0.262) data 0.000 (0.001) loss 1.8033 (1.9048) acc 56.2500 (52.1213) (mean 51.9306 many 51.9306 med nan few nan) lr 9.0451e-03 elapsed 0:04:07 eta 0:09:31\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [320/625] time 0.265 (0.262) data 0.000 (0.001) loss 1.8033 (1.9048) acc 56.2500 (52.1213) (mean 51.9306 many 51.9306 med nan few nan) lr 9.0451e-03 elapsed 0:04:07 eta 0:09:31\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [340/625] time 0.265 (0.262) data 0.000 (0.001) loss 2.0732 (1.8990) acc 46.8750 (52.6110) (mean 53.0810 many 53.0810 med nan few nan) lr 9.0451e-03 elapsed 0:04:13 eta 0:09:26\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [340/625] time 0.265 (0.262) data 0.000 (0.001) loss 2.0732 (1.8990) acc 46.8750 (52.6110) (mean 53.0810 many 53.0810 med nan few nan) lr 9.0451e-03 elapsed 0:04:13 eta 0:09:26\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [360/625] time 0.269 (0.262) data 0.000 (0.001) loss 2.3861 (1.9511) acc 39.0625 (49.9705) (mean 50.6524 many 50.6524 med nan few nan) lr 9.0451e-03 elapsed 0:04:18 eta 0:09:21\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [360/625] time 0.269 (0.262) data 0.000 (0.001) loss 2.3861 (1.9511) acc 39.0625 (49.9705) (mean 50.6524 many 50.6524 med nan few nan) lr 9.0451e-03 elapsed 0:04:18 eta 0:09:21\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [380/625] time 0.265 (0.262) data 0.000 (0.001) loss 2.1617 (1.9071) acc 51.5625 (53.1791) (mean 53.4251 many 53.4251 med nan few nan) lr 9.0451e-03 elapsed 0:04:23 eta 0:09:16\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [380/625] time 0.265 (0.262) data 0.000 (0.001) loss 2.1617 (1.9071) acc 51.5625 (53.1791) (mean 53.4251 many 53.4251 med nan few nan) lr 9.0451e-03 elapsed 0:04:23 eta 0:09:16\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [400/625] time 0.267 (0.262) data 0.000 (0.001) loss 1.7957 (1.9313) acc 59.3750 (53.3110) (mean 53.1915 many 53.1915 med nan few nan) lr 9.0451e-03 elapsed 0:04:29 eta 0:09:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [400/625] time 0.267 (0.262) data 0.000 (0.001) loss 1.7957 (1.9313) acc 59.3750 (53.3110) (mean 53.1915 many 53.1915 med nan few nan) lr 9.0451e-03 elapsed 0:04:29 eta 0:09:11\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [420/625] time 0.264 (0.263) data 0.000 (0.001) loss 1.9796 (1.8981) acc 48.4375 (52.9713) (mean 53.3239 many 53.3239 med nan few nan) lr 9.0451e-03 elapsed 0:04:34 eta 0:09:06\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [420/625] time 0.264 (0.263) data 0.000 (0.001) loss 1.9796 (1.8981) acc 48.4375 (52.9713) (mean 53.3239 many 53.3239 med nan few nan) lr 9.0451e-03 elapsed 0:04:34 eta 0:09:06\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [440/625] time 0.262 (0.263) data 0.000 (0.001) loss 1.9493 (1.8735) acc 54.6875 (53.8685) (mean 54.2743 many 54.2743 med nan few nan) lr 9.0451e-03 elapsed 0:04:39 eta 0:09:00\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [440/625] time 0.262 (0.263) data 0.000 (0.001) loss 1.9493 (1.8735) acc 54.6875 (53.8685) (mean 54.2743 many 54.2743 med nan few nan) lr 9.0451e-03 elapsed 0:04:39 eta 0:09:00\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [460/625] time 0.264 (0.263) data 0.000 (0.001) loss 1.8973 (1.8748) acc 51.5625 (53.1411) (mean 53.6096 many 53.6096 med nan few nan) lr 9.0451e-03 elapsed 0:04:45 eta 0:08:55\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [460/625] time 0.264 (0.263) data 0.000 (0.001) loss 1.8973 (1.8748) acc 51.5625 (53.1411) (mean 53.6096 many 53.6096 med nan few nan) lr 9.0451e-03 elapsed 0:04:45 eta 0:08:55\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [480/625] time 0.269 (0.263) data 0.000 (0.001) loss 1.6537 (1.8516) acc 57.8125 (54.3092) (mean 54.1996 many 54.1996 med nan few nan) lr 9.0451e-03 elapsed 0:04:50 eta 0:08:50\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [480/625] time 0.269 (0.263) data 0.000 (0.001) loss 1.6537 (1.8516) acc 57.8125 (54.3092) (mean 54.1996 many 54.1996 med nan few nan) lr 9.0451e-03 elapsed 0:04:50 eta 0:08:50\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [500/625] time 0.269 (0.263) data 0.004 (0.001) loss 1.7263 (1.8810) acc 59.3750 (53.2568) (mean 53.2561 many 53.2561 med nan few nan) lr 9.0451e-03 elapsed 0:04:55 eta 0:08:45\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [500/625] time 0.269 (0.263) data 0.004 (0.001) loss 1.7263 (1.8810) acc 59.3750 (53.2568) (mean 53.2561 many 53.2561 med nan few nan) lr 9.0451e-03 elapsed 0:04:55 eta 0:08:45\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [520/625] time 0.263 (0.263) data 0.000 (0.001) loss 2.3714 (1.8108) acc 40.6250 (54.2081) (mean 54.4504 many 54.4504 med nan few nan) lr 9.0451e-03 elapsed 0:05:00 eta 0:08:40\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [520/625] time 0.263 (0.263) data 0.000 (0.001) loss 2.3714 (1.8108) acc 40.6250 (54.2081) (mean 54.4504 many 54.4504 med nan few nan) lr 9.0451e-03 elapsed 0:05:00 eta 0:08:40\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [540/625] time 0.276 (0.263) data 0.000 (0.001) loss 2.0475 (1.8311) acc 45.3125 (55.2104) (mean 55.7463 many 55.7463 med nan few nan) lr 9.0451e-03 elapsed 0:05:06 eta 0:08:35\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [540/625] time 0.276 (0.263) data 0.000 (0.001) loss 2.0475 (1.8311) acc 45.3125 (55.2104) (mean 55.7463 many 55.7463 med nan few nan) lr 9.0451e-03 elapsed 0:05:06 eta 0:08:35\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [560/625] time 0.273 (0.263) data 0.000 (0.001) loss 1.9136 (1.8190) acc 50.0000 (56.0938) (mean 56.0153 many 56.0153 med nan few nan) lr 9.0451e-03 elapsed 0:05:11 eta 0:08:30\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [560/625] time 0.273 (0.263) data 0.000 (0.001) loss 1.9136 (1.8190) acc 50.0000 (56.0938) (mean 56.0153 many 56.0153 med nan few nan) lr 9.0451e-03 elapsed 0:05:11 eta 0:08:30\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [580/625] time 0.264 (0.263) data 0.000 (0.001) loss 2.0175 (1.8480) acc 50.0000 (53.1864) (mean 53.6081 many 53.6081 med nan few nan) lr 9.0451e-03 elapsed 0:05:16 eta 0:08:24\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [580/625] time 0.264 (0.263) data 0.000 (0.001) loss 2.0175 (1.8480) acc 50.0000 (53.1864) (mean 53.6081 many 53.6081 med nan few nan) lr 9.0451e-03 elapsed 0:05:16 eta 0:08:24\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [600/625] time 0.265 (0.263) data 0.000 (0.001) loss 1.8918 (1.8567) acc 53.1250 (53.3982) (mean 53.3945 many 53.3945 med nan few nan) lr 9.0451e-03 elapsed 0:05:22 eta 0:08:19\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [600/625] time 0.265 (0.263) data 0.000 (0.001) loss 1.8918 (1.8567) acc 53.1250 (53.3982) (mean 53.3945 many 53.3945 med nan few nan) lr 9.0451e-03 elapsed 0:05:22 eta 0:08:19\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [620/625] time 0.267 (0.263) data 0.000 (0.001) loss 2.0008 (1.9182) acc 53.1250 (52.3740) (mean 52.9492 many 52.9492 med nan few nan) lr 9.0451e-03 elapsed 0:05:27 eta 0:08:14\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [620/625] time 0.267 (0.263) data 0.000 (0.001) loss 2.0008 (1.9182) acc 53.1250 (52.3740) (mean 52.9492 many 52.9492 med nan few nan) lr 9.0451e-03 elapsed 0:05:27 eta 0:08:14\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [625/625] time 0.265 (0.263) data 0.000 (0.001) loss 2.2592 (1.9312) acc 48.4375 (52.6828) (mean 53.0775 many 53.0775 med nan few nan) lr 9.0451e-03 elapsed 0:05:29 eta 0:08:13\u001b[0m\n",
      "\u001b[37mepoch [2/5] batch [625/625] time 0.265 (0.263) data 0.000 (0.001) loss 2.2592 (1.9312) acc 48.4375 (52.6828) (mean 53.0775 many 53.0775 med nan few nan) lr 9.0451e-03 elapsed 0:05:29 eta 0:08:13\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [20/625] time 0.267 (0.264) data 0.000 (0.002) loss 1.9871 (1.8685) acc 50.0000 (55.0428) (mean 54.9129 many 54.9129 med nan few nan) lr 6.5451e-03 elapsed 0:05:34 eta 0:08:08\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [20/625] time 0.267 (0.264) data 0.000 (0.002) loss 1.9871 (1.8685) acc 50.0000 (55.0428) (mean 54.9129 many 54.9129 med nan few nan) lr 6.5451e-03 elapsed 0:05:34 eta 0:08:08\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [40/625] time 0.264 (0.264) data 0.000 (0.002) loss 1.8654 (1.8177) acc 54.6875 (55.5299) (mean 55.7586 many 55.7586 med nan few nan) lr 6.5451e-03 elapsed 0:05:40 eta 0:08:03\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [40/625] time 0.264 (0.264) data 0.000 (0.002) loss 1.8654 (1.8177) acc 54.6875 (55.5299) (mean 55.7586 many 55.7586 med nan few nan) lr 6.5451e-03 elapsed 0:05:40 eta 0:08:03\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [60/625] time 0.271 (0.264) data 0.004 (0.002) loss 2.0163 (1.7579) acc 53.1250 (57.4559) (mean 57.6292 many 57.6292 med nan few nan) lr 6.5451e-03 elapsed 0:05:45 eta 0:07:58\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [60/625] time 0.271 (0.264) data 0.004 (0.002) loss 2.0163 (1.7579) acc 53.1250 (57.4559) (mean 57.6292 many 57.6292 med nan few nan) lr 6.5451e-03 elapsed 0:05:45 eta 0:07:58\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [80/625] time 0.265 (0.264) data 0.000 (0.002) loss 1.9285 (1.7567) acc 48.4375 (55.2699) (mean 55.8198 many 55.8198 med nan few nan) lr 6.5451e-03 elapsed 0:05:50 eta 0:07:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [80/625] time 0.265 (0.264) data 0.000 (0.002) loss 1.9285 (1.7567) acc 48.4375 (55.2699) (mean 55.8198 many 55.8198 med nan few nan) lr 6.5451e-03 elapsed 0:05:50 eta 0:07:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [100/625] time 0.268 (0.264) data 0.000 (0.002) loss 1.8084 (1.8354) acc 56.2500 (54.0243) (mean 53.8944 many 53.8944 med nan few nan) lr 6.5451e-03 elapsed 0:05:56 eta 0:07:48\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [100/625] time 0.268 (0.264) data 0.000 (0.002) loss 1.8084 (1.8354) acc 56.2500 (54.0243) (mean 53.8944 many 53.8944 med nan few nan) lr 6.5451e-03 elapsed 0:05:56 eta 0:07:48\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [120/625] time 0.268 (0.264) data 0.000 (0.001) loss 1.9865 (1.8400) acc 54.6875 (55.0013) (mean 54.4475 many 54.4475 med nan few nan) lr 6.5451e-03 elapsed 0:06:01 eta 0:07:43\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [120/625] time 0.268 (0.264) data 0.000 (0.001) loss 1.9865 (1.8400) acc 54.6875 (55.0013) (mean 54.4475 many 54.4475 med nan few nan) lr 6.5451e-03 elapsed 0:06:01 eta 0:07:43\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [140/625] time 0.264 (0.264) data 0.000 (0.001) loss 1.9723 (1.8206) acc 54.6875 (56.0607) (mean 55.9965 many 55.9965 med nan few nan) lr 6.5451e-03 elapsed 0:06:06 eta 0:07:37\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [140/625] time 0.264 (0.264) data 0.000 (0.001) loss 1.9723 (1.8206) acc 54.6875 (56.0607) (mean 55.9965 many 55.9965 med nan few nan) lr 6.5451e-03 elapsed 0:06:06 eta 0:07:37\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [160/625] time 0.267 (0.264) data 0.000 (0.001) loss 1.8440 (1.8540) acc 56.2500 (55.9899) (mean 55.7026 many 55.7026 med nan few nan) lr 6.5451e-03 elapsed 0:06:12 eta 0:07:32\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [160/625] time 0.267 (0.264) data 0.000 (0.001) loss 1.8440 (1.8540) acc 56.2500 (55.9899) (mean 55.7026 many 55.7026 med nan few nan) lr 6.5451e-03 elapsed 0:06:12 eta 0:07:32\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [180/625] time 0.263 (0.264) data 0.000 (0.001) loss 1.7980 (1.8745) acc 50.0000 (52.8415) (mean 53.3698 many 53.3698 med nan few nan) lr 6.5451e-03 elapsed 0:06:17 eta 0:07:27\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [180/625] time 0.263 (0.264) data 0.000 (0.001) loss 1.7980 (1.8745) acc 50.0000 (52.8415) (mean 53.3698 many 53.3698 med nan few nan) lr 6.5451e-03 elapsed 0:06:17 eta 0:07:27\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [200/625] time 0.275 (0.264) data 0.002 (0.001) loss 1.8196 (1.8400) acc 56.2500 (54.6823) (mean 54.8197 many 54.8197 med nan few nan) lr 6.5451e-03 elapsed 0:06:23 eta 0:07:22\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [200/625] time 0.275 (0.264) data 0.002 (0.001) loss 1.8196 (1.8400) acc 56.2500 (54.6823) (mean 54.8197 many 54.8197 med nan few nan) lr 6.5451e-03 elapsed 0:06:23 eta 0:07:22\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [220/625] time 0.263 (0.264) data 0.000 (0.001) loss 1.9931 (1.8477) acc 50.0000 (54.5749) (mean 54.9772 many 54.9772 med nan few nan) lr 6.5451e-03 elapsed 0:06:28 eta 0:07:17\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [220/625] time 0.263 (0.264) data 0.000 (0.001) loss 1.9931 (1.8477) acc 50.0000 (54.5749) (mean 54.9772 many 54.9772 med nan few nan) lr 6.5451e-03 elapsed 0:06:28 eta 0:07:17\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [240/625] time 0.269 (0.264) data 0.001 (0.001) loss 1.6097 (1.8174) acc 67.1875 (56.1214) (mean 55.8656 many 55.8656 med nan few nan) lr 6.5451e-03 elapsed 0:06:33 eta 0:07:11\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [240/625] time 0.269 (0.264) data 0.001 (0.001) loss 1.6097 (1.8174) acc 67.1875 (56.1214) (mean 55.8656 many 55.8656 med nan few nan) lr 6.5451e-03 elapsed 0:06:33 eta 0:07:11\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [260/625] time 0.267 (0.264) data 0.000 (0.001) loss 1.8859 (1.7822) acc 46.8750 (56.3155) (mean 56.6041 many 56.6041 med nan few nan) lr 6.5451e-03 elapsed 0:06:39 eta 0:07:06\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [260/625] time 0.267 (0.264) data 0.000 (0.001) loss 1.8859 (1.7822) acc 46.8750 (56.3155) (mean 56.6041 many 56.6041 med nan few nan) lr 6.5451e-03 elapsed 0:06:39 eta 0:07:06\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [280/625] time 0.265 (0.264) data 0.000 (0.001) loss 1.6702 (1.8467) acc 57.8125 (55.4028) (mean 55.9544 many 55.9544 med nan few nan) lr 6.5451e-03 elapsed 0:06:44 eta 0:07:01\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [280/625] time 0.265 (0.264) data 0.000 (0.001) loss 1.6702 (1.8467) acc 57.8125 (55.4028) (mean 55.9544 many 55.9544 med nan few nan) lr 6.5451e-03 elapsed 0:06:44 eta 0:07:01\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [300/625] time 0.265 (0.264) data 0.000 (0.001) loss 2.0822 (1.8594) acc 42.1875 (53.1139) (mean 53.8869 many 53.8869 med nan few nan) lr 6.5451e-03 elapsed 0:06:49 eta 0:06:56\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [300/625] time 0.265 (0.264) data 0.000 (0.001) loss 2.0822 (1.8594) acc 42.1875 (53.1139) (mean 53.8869 many 53.8869 med nan few nan) lr 6.5451e-03 elapsed 0:06:49 eta 0:06:56\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [320/625] time 0.266 (0.264) data 0.000 (0.001) loss 2.1227 (1.8865) acc 45.3125 (53.2851) (mean 53.9799 many 53.9799 med nan few nan) lr 6.5451e-03 elapsed 0:06:55 eta 0:06:51\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [320/625] time 0.266 (0.264) data 0.000 (0.001) loss 2.1227 (1.8865) acc 45.3125 (53.2851) (mean 53.9799 many 53.9799 med nan few nan) lr 6.5451e-03 elapsed 0:06:55 eta 0:06:51\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [340/625] time 0.269 (0.264) data 0.000 (0.001) loss 1.5303 (1.7921) acc 62.5000 (55.9482) (mean 55.5883 many 55.5883 med nan few nan) lr 6.5451e-03 elapsed 0:07:00 eta 0:06:45\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [340/625] time 0.269 (0.264) data 0.000 (0.001) loss 1.5303 (1.7921) acc 62.5000 (55.9482) (mean 55.5883 many 55.5883 med nan few nan) lr 6.5451e-03 elapsed 0:07:00 eta 0:06:45\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [360/625] time 0.263 (0.264) data 0.000 (0.001) loss 1.8970 (1.8425) acc 51.5625 (53.8835) (mean 54.0553 many 54.0553 med nan few nan) lr 6.5451e-03 elapsed 0:07:05 eta 0:06:40\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [360/625] time 0.263 (0.264) data 0.000 (0.001) loss 1.8970 (1.8425) acc 51.5625 (53.8835) (mean 54.0553 many 54.0553 med nan few nan) lr 6.5451e-03 elapsed 0:07:05 eta 0:06:40\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [380/625] time 0.272 (0.265) data 0.000 (0.001) loss 1.9267 (1.8218) acc 50.0000 (55.3186) (mean 55.3397 many 55.3397 med nan few nan) lr 6.5451e-03 elapsed 0:07:11 eta 0:06:35\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [380/625] time 0.272 (0.265) data 0.000 (0.001) loss 1.9267 (1.8218) acc 50.0000 (55.3186) (mean 55.3397 many 55.3397 med nan few nan) lr 6.5451e-03 elapsed 0:07:11 eta 0:06:35\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [400/625] time 0.273 (0.265) data 0.000 (0.001) loss 2.0209 (1.8129) acc 45.3125 (55.3110) (mean 55.5331 many 55.5331 med nan few nan) lr 6.5451e-03 elapsed 0:07:16 eta 0:06:30\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [400/625] time 0.273 (0.265) data 0.000 (0.001) loss 2.0209 (1.8129) acc 45.3125 (55.3110) (mean 55.5331 many 55.5331 med nan few nan) lr 6.5451e-03 elapsed 0:07:16 eta 0:06:30\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [420/625] time 0.265 (0.265) data 0.000 (0.001) loss 2.1963 (1.8381) acc 46.8750 (55.5172) (mean 55.7706 many 55.7706 med nan few nan) lr 6.5451e-03 elapsed 0:07:22 eta 0:06:25\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [420/625] time 0.265 (0.265) data 0.000 (0.001) loss 2.1963 (1.8381) acc 46.8750 (55.5172) (mean 55.7706 many 55.7706 med nan few nan) lr 6.5451e-03 elapsed 0:07:22 eta 0:06:25\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [440/625] time 0.273 (0.265) data 0.000 (0.001) loss 2.0576 (1.8590) acc 48.4375 (54.7516) (mean 55.1328 many 55.1328 med nan few nan) lr 6.5451e-03 elapsed 0:07:27 eta 0:06:19\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [440/625] time 0.273 (0.265) data 0.000 (0.001) loss 2.0576 (1.8590) acc 48.4375 (54.7516) (mean 55.1328 many 55.1328 med nan few nan) lr 6.5451e-03 elapsed 0:07:27 eta 0:06:19\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [460/625] time 0.264 (0.265) data 0.000 (0.001) loss 2.0897 (1.8820) acc 46.8750 (53.7536) (mean 54.3430 many 54.3430 med nan few nan) lr 6.5451e-03 elapsed 0:07:32 eta 0:06:14\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [460/625] time 0.264 (0.265) data 0.000 (0.001) loss 2.0897 (1.8820) acc 46.8750 (53.7536) (mean 54.3430 many 54.3430 med nan few nan) lr 6.5451e-03 elapsed 0:07:32 eta 0:06:14\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [480/625] time 0.268 (0.265) data 0.000 (0.001) loss 1.9486 (1.8628) acc 43.7500 (55.2829) (mean 55.8924 many 55.8924 med nan few nan) lr 6.5451e-03 elapsed 0:07:38 eta 0:06:09\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [480/625] time 0.268 (0.265) data 0.000 (0.001) loss 1.9486 (1.8628) acc 43.7500 (55.2829) (mean 55.8924 many 55.8924 med nan few nan) lr 6.5451e-03 elapsed 0:07:38 eta 0:06:09\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [500/625] time 0.267 (0.265) data 0.000 (0.001) loss 1.8283 (1.8729) acc 56.2500 (53.8494) (mean 54.0326 many 54.0326 med nan few nan) lr 6.5451e-03 elapsed 0:07:43 eta 0:06:04\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [500/625] time 0.267 (0.265) data 0.000 (0.001) loss 1.8283 (1.8729) acc 56.2500 (53.8494) (mean 54.0326 many 54.0326 med nan few nan) lr 6.5451e-03 elapsed 0:07:43 eta 0:06:04\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [520/625] time 0.275 (0.265) data 0.000 (0.001) loss 1.8536 (1.8002) acc 56.2500 (56.0913) (mean 55.9861 many 55.9861 med nan few nan) lr 6.5451e-03 elapsed 0:07:48 eta 0:05:58\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [520/625] time 0.275 (0.265) data 0.000 (0.001) loss 1.8536 (1.8002) acc 56.2500 (56.0913) (mean 55.9861 many 55.9861 med nan few nan) lr 6.5451e-03 elapsed 0:07:48 eta 0:05:58\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [540/625] time 0.274 (0.265) data 0.000 (0.001) loss 2.0027 (1.7816) acc 54.6875 (56.1141) (mean 56.4842 many 56.4842 med nan few nan) lr 6.5451e-03 elapsed 0:07:54 eta 0:05:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [540/625] time 0.274 (0.265) data 0.000 (0.001) loss 2.0027 (1.7816) acc 54.6875 (56.1141) (mean 56.4842 many 56.4842 med nan few nan) lr 6.5451e-03 elapsed 0:07:54 eta 0:05:53\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [560/625] time 0.266 (0.265) data 0.000 (0.001) loss 2.0076 (1.8043) acc 46.8750 (55.4593) (mean 55.7230 many 55.7230 med nan few nan) lr 6.5451e-03 elapsed 0:07:59 eta 0:05:48\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [560/625] time 0.266 (0.265) data 0.000 (0.001) loss 2.0076 (1.8043) acc 46.8750 (55.4593) (mean 55.7230 many 55.7230 med nan few nan) lr 6.5451e-03 elapsed 0:07:59 eta 0:05:48\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [580/625] time 0.272 (0.265) data 0.000 (0.001) loss 2.1032 (1.8695) acc 50.0000 (54.4964) (mean 55.0303 many 55.0303 med nan few nan) lr 6.5451e-03 elapsed 0:08:05 eta 0:05:43\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [580/625] time 0.272 (0.265) data 0.000 (0.001) loss 2.1032 (1.8695) acc 50.0000 (54.4964) (mean 55.0303 many 55.0303 med nan few nan) lr 6.5451e-03 elapsed 0:08:05 eta 0:05:43\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [600/625] time 0.267 (0.265) data 0.000 (0.001) loss 2.0838 (1.8128) acc 48.4375 (55.6877) (mean 55.8744 many 55.8744 med nan few nan) lr 6.5451e-03 elapsed 0:08:10 eta 0:05:37\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [600/625] time 0.267 (0.265) data 0.000 (0.001) loss 2.0838 (1.8128) acc 48.4375 (55.6877) (mean 55.8744 many 55.8744 med nan few nan) lr 6.5451e-03 elapsed 0:08:10 eta 0:05:37\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [620/625] time 0.275 (0.265) data 0.002 (0.001) loss 2.4615 (1.8867) acc 37.5000 (53.8900) (mean 54.2680 many 54.2680 med nan few nan) lr 6.5451e-03 elapsed 0:08:15 eta 0:05:32\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [620/625] time 0.275 (0.265) data 0.002 (0.001) loss 2.4615 (1.8867) acc 37.5000 (53.8900) (mean 54.2680 many 54.2680 med nan few nan) lr 6.5451e-03 elapsed 0:08:15 eta 0:05:32\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [625/625] time 0.268 (0.265) data 0.000 (0.001) loss 1.5080 (1.8238) acc 68.7500 (56.3116) (mean 56.0722 many 56.0722 med nan few nan) lr 6.5451e-03 elapsed 0:08:17 eta 0:05:31\u001b[0m\n",
      "\u001b[37mepoch [3/5] batch [625/625] time 0.268 (0.265) data 0.000 (0.001) loss 1.5080 (1.8238) acc 68.7500 (56.3116) (mean 56.0722 many 56.0722 med nan few nan) lr 6.5451e-03 elapsed 0:08:17 eta 0:05:31\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [20/625] time 0.273 (0.265) data 0.000 (0.001) loss 2.2187 (1.8822) acc 50.0000 (54.4425) (mean 54.7961 many 54.7961 med nan few nan) lr 3.4549e-03 elapsed 0:08:23 eta 0:05:26\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [20/625] time 0.273 (0.265) data 0.000 (0.001) loss 2.2187 (1.8822) acc 50.0000 (54.4425) (mean 54.7961 many 54.7961 med nan few nan) lr 3.4549e-03 elapsed 0:08:23 eta 0:05:26\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [40/625] time 0.276 (0.265) data 0.002 (0.001) loss 1.9448 (1.8538) acc 54.6875 (56.8863) (mean 57.0551 many 57.0551 med nan few nan) lr 3.4549e-03 elapsed 0:08:28 eta 0:05:21\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [40/625] time 0.276 (0.265) data 0.002 (0.001) loss 1.9448 (1.8538) acc 54.6875 (56.8863) (mean 57.0551 many 57.0551 med nan few nan) lr 3.4549e-03 elapsed 0:08:28 eta 0:05:21\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [60/625] time 0.272 (0.265) data 0.000 (0.001) loss 2.0667 (1.8453) acc 46.8750 (56.0364) (mean 56.8006 many 56.8006 med nan few nan) lr 3.4549e-03 elapsed 0:08:33 eta 0:05:15\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [60/625] time 0.272 (0.265) data 0.000 (0.001) loss 2.0667 (1.8453) acc 46.8750 (56.0364) (mean 56.8006 many 56.8006 med nan few nan) lr 3.4549e-03 elapsed 0:08:33 eta 0:05:15\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [80/625] time 0.266 (0.265) data 0.000 (0.001) loss 1.9761 (1.8118) acc 51.5625 (55.4396) (mean 55.8939 many 55.8939 med nan few nan) lr 3.4549e-03 elapsed 0:08:39 eta 0:05:10\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [80/625] time 0.266 (0.265) data 0.000 (0.001) loss 1.9761 (1.8118) acc 51.5625 (55.4396) (mean 55.8939 many 55.8939 med nan few nan) lr 3.4549e-03 elapsed 0:08:39 eta 0:05:10\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [100/625] time 0.265 (0.265) data 0.000 (0.001) loss 1.7356 (1.8003) acc 59.3750 (56.1442) (mean 56.3071 many 56.3071 med nan few nan) lr 3.4549e-03 elapsed 0:08:44 eta 0:05:05\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [100/625] time 0.265 (0.265) data 0.000 (0.001) loss 1.7356 (1.8003) acc 59.3750 (56.1442) (mean 56.3071 many 56.3071 med nan few nan) lr 3.4549e-03 elapsed 0:08:44 eta 0:05:05\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [120/625] time 0.270 (0.265) data 0.000 (0.001) loss 1.8611 (1.7589) acc 54.6875 (57.1553) (mean 57.2031 many 57.2031 med nan few nan) lr 3.4549e-03 elapsed 0:08:49 eta 0:04:59\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [120/625] time 0.270 (0.265) data 0.000 (0.001) loss 1.8611 (1.7589) acc 54.6875 (57.1553) (mean 57.2031 many 57.2031 med nan few nan) lr 3.4549e-03 elapsed 0:08:49 eta 0:04:59\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [140/625] time 0.265 (0.266) data 0.000 (0.001) loss 1.5953 (1.7351) acc 60.9375 (58.0875) (mean 57.7803 many 57.7803 med nan few nan) lr 3.4549e-03 elapsed 0:08:55 eta 0:04:54\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [140/625] time 0.265 (0.266) data 0.000 (0.001) loss 1.5953 (1.7351) acc 60.9375 (58.0875) (mean 57.7803 many 57.7803 med nan few nan) lr 3.4549e-03 elapsed 0:08:55 eta 0:04:54\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [160/625] time 0.266 (0.266) data 0.000 (0.001) loss 2.0002 (1.8168) acc 43.7500 (54.4175) (mean 54.9907 many 54.9907 med nan few nan) lr 3.4549e-03 elapsed 0:09:00 eta 0:04:49\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [160/625] time 0.266 (0.266) data 0.000 (0.001) loss 2.0002 (1.8168) acc 43.7500 (54.4175) (mean 54.9907 many 54.9907 med nan few nan) lr 3.4549e-03 elapsed 0:09:00 eta 0:04:49\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [180/625] time 0.273 (0.266) data 0.003 (0.001) loss 1.8932 (1.8053) acc 54.6875 (56.1170) (mean 56.2861 many 56.2861 med nan few nan) lr 3.4549e-03 elapsed 0:09:05 eta 0:04:44\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [180/625] time 0.273 (0.266) data 0.003 (0.001) loss 1.8932 (1.8053) acc 54.6875 (56.1170) (mean 56.2861 many 56.2861 med nan few nan) lr 3.4549e-03 elapsed 0:09:05 eta 0:04:44\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [200/625] time 0.271 (0.266) data 0.000 (0.001) loss 2.0337 (1.7762) acc 46.8750 (56.3232) (mean 56.3640 many 56.3640 med nan few nan) lr 3.4549e-03 elapsed 0:09:11 eta 0:04:38\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [200/625] time 0.271 (0.266) data 0.000 (0.001) loss 2.0337 (1.7762) acc 46.8750 (56.3232) (mean 56.3640 many 56.3640 med nan few nan) lr 3.4549e-03 elapsed 0:09:11 eta 0:04:38\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [220/625] time 0.267 (0.266) data 0.000 (0.001) loss 1.8242 (1.7403) acc 57.8125 (57.9998) (mean 58.2172 many 58.2172 med nan few nan) lr 3.4549e-03 elapsed 0:09:16 eta 0:04:33\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [220/625] time 0.267 (0.266) data 0.000 (0.001) loss 1.8242 (1.7403) acc 57.8125 (57.9998) (mean 58.2172 many 58.2172 med nan few nan) lr 3.4549e-03 elapsed 0:09:16 eta 0:04:33\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [240/625] time 0.269 (0.266) data 0.000 (0.001) loss 1.8748 (1.7270) acc 48.4375 (57.6032) (mean 57.6614 many 57.6614 med nan few nan) lr 3.4549e-03 elapsed 0:09:22 eta 0:04:28\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [240/625] time 0.269 (0.266) data 0.000 (0.001) loss 1.8748 (1.7270) acc 48.4375 (57.6032) (mean 57.6614 many 57.6614 med nan few nan) lr 3.4549e-03 elapsed 0:09:22 eta 0:04:28\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [260/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.7122 (1.7827) acc 56.2500 (55.5209) (mean 55.8628 many 55.8628 med nan few nan) lr 3.4549e-03 elapsed 0:09:27 eta 0:04:22\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [260/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.7122 (1.7827) acc 56.2500 (55.5209) (mean 55.8628 many 55.8628 med nan few nan) lr 3.4549e-03 elapsed 0:09:27 eta 0:04:22\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [280/625] time 0.266 (0.266) data 0.001 (0.001) loss 1.5136 (1.7067) acc 67.1875 (58.4103) (mean 58.2193 many 58.2193 med nan few nan) lr 3.4549e-03 elapsed 0:09:32 eta 0:04:17\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [280/625] time 0.266 (0.266) data 0.001 (0.001) loss 1.5136 (1.7067) acc 67.1875 (58.4103) (mean 58.2193 many 58.2193 med nan few nan) lr 3.4549e-03 elapsed 0:09:32 eta 0:04:17\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [300/625] time 0.265 (0.266) data 0.000 (0.001) loss 1.6332 (1.8067) acc 57.8125 (54.8241) (mean 55.5837 many 55.5837 med nan few nan) lr 3.4549e-03 elapsed 0:09:38 eta 0:04:12\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [300/625] time 0.265 (0.266) data 0.000 (0.001) loss 1.6332 (1.8067) acc 57.8125 (54.8241) (mean 55.5837 many 55.5837 med nan few nan) lr 3.4549e-03 elapsed 0:09:38 eta 0:04:12\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [320/625] time 0.271 (0.266) data 0.003 (0.001) loss 1.7739 (1.8099) acc 51.5625 (55.2870) (mean 55.4425 many 55.4425 med nan few nan) lr 3.4549e-03 elapsed 0:09:43 eta 0:04:07\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [320/625] time 0.271 (0.266) data 0.003 (0.001) loss 1.7739 (1.8099) acc 51.5625 (55.2870) (mean 55.4425 many 55.4425 med nan few nan) lr 3.4549e-03 elapsed 0:09:43 eta 0:04:07\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [340/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.9733 (1.7882) acc 51.5625 (56.6991) (mean 56.6807 many 56.6807 med nan few nan) lr 3.4549e-03 elapsed 0:09:48 eta 0:04:01\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [340/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.9733 (1.7882) acc 51.5625 (56.6991) (mean 56.6807 many 56.6807 med nan few nan) lr 3.4549e-03 elapsed 0:09:48 eta 0:04:01\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [360/625] time 0.273 (0.266) data 0.000 (0.001) loss 1.6978 (1.7543) acc 59.3750 (58.2912) (mean 57.8535 many 57.8535 med nan few nan) lr 3.4549e-03 elapsed 0:09:54 eta 0:03:56\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [360/625] time 0.273 (0.266) data 0.000 (0.001) loss 1.6978 (1.7543) acc 59.3750 (58.2912) (mean 57.8535 many 57.8535 med nan few nan) lr 3.4549e-03 elapsed 0:09:54 eta 0:03:56\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [380/625] time 0.269 (0.266) data 0.000 (0.001) loss 1.3433 (1.7163) acc 62.5000 (58.5542) (mean 58.1665 many 58.1665 med nan few nan) lr 3.4549e-03 elapsed 0:09:59 eta 0:03:51\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [380/625] time 0.269 (0.266) data 0.000 (0.001) loss 1.3433 (1.7163) acc 62.5000 (58.5542) (mean 58.1665 many 58.1665 med nan few nan) lr 3.4549e-03 elapsed 0:09:59 eta 0:03:51\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [400/625] time 0.272 (0.266) data 0.000 (0.001) loss 1.7201 (1.7330) acc 64.0625 (58.5254) (mean 58.2393 many 58.2393 med nan few nan) lr 3.4549e-03 elapsed 0:10:04 eta 0:03:45\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [400/625] time 0.272 (0.266) data 0.000 (0.001) loss 1.7201 (1.7330) acc 64.0625 (58.5254) (mean 58.2393 many 58.2393 med nan few nan) lr 3.4549e-03 elapsed 0:10:04 eta 0:03:45\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [420/625] time 0.266 (0.266) data 0.000 (0.001) loss 1.6257 (1.7160) acc 65.6250 (58.9247) (mean 58.4629 many 58.4629 med nan few nan) lr 3.4549e-03 elapsed 0:10:10 eta 0:03:40\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [420/625] time 0.266 (0.266) data 0.000 (0.001) loss 1.6257 (1.7160) acc 65.6250 (58.9247) (mean 58.4629 many 58.4629 med nan few nan) lr 3.4549e-03 elapsed 0:10:10 eta 0:03:40\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [440/625] time 0.267 (0.266) data 0.000 (0.001) loss 2.0204 (1.7783) acc 43.7500 (57.0300) (mean 57.4934 many 57.4934 med nan few nan) lr 3.4549e-03 elapsed 0:10:15 eta 0:03:35\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [440/625] time 0.267 (0.266) data 0.000 (0.001) loss 2.0204 (1.7783) acc 43.7500 (57.0300) (mean 57.4934 many 57.4934 med nan few nan) lr 3.4549e-03 elapsed 0:10:15 eta 0:03:35\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [460/625] time 0.275 (0.266) data 0.000 (0.001) loss 1.7661 (1.7930) acc 59.3750 (55.8653) (mean 56.1075 many 56.1075 med nan few nan) lr 3.4549e-03 elapsed 0:10:21 eta 0:03:30\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [460/625] time 0.275 (0.266) data 0.000 (0.001) loss 1.7661 (1.7930) acc 59.3750 (55.8653) (mean 56.1075 many 56.1075 med nan few nan) lr 3.4549e-03 elapsed 0:10:21 eta 0:03:30\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [480/625] time 0.267 (0.266) data 0.001 (0.001) loss 1.7516 (1.7831) acc 56.2500 (57.2669) (mean 57.4601 many 57.4601 med nan few nan) lr 3.4549e-03 elapsed 0:10:26 eta 0:03:24\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [480/625] time 0.267 (0.266) data 0.001 (0.001) loss 1.7516 (1.7831) acc 56.2500 (57.2669) (mean 57.4601 many 57.4601 med nan few nan) lr 3.4549e-03 elapsed 0:10:26 eta 0:03:24\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [500/625] time 0.264 (0.266) data 0.000 (0.001) loss 1.6528 (1.7579) acc 60.9375 (58.8359) (mean 58.5305 many 58.5305 med nan few nan) lr 3.4549e-03 elapsed 0:10:31 eta 0:03:19\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [500/625] time 0.264 (0.266) data 0.000 (0.001) loss 1.6528 (1.7579) acc 60.9375 (58.8359) (mean 58.5305 many 58.5305 med nan few nan) lr 3.4549e-03 elapsed 0:10:31 eta 0:03:19\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [520/625] time 0.262 (0.266) data 0.000 (0.001) loss 1.7980 (1.7998) acc 53.1250 (55.9845) (mean 56.0767 many 56.0767 med nan few nan) lr 3.4549e-03 elapsed 0:10:37 eta 0:03:14\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [520/625] time 0.262 (0.266) data 0.000 (0.001) loss 1.7980 (1.7998) acc 53.1250 (55.9845) (mean 56.0767 many 56.0767 med nan few nan) lr 3.4549e-03 elapsed 0:10:37 eta 0:03:14\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [540/625] time 0.266 (0.266) data 0.000 (0.001) loss 1.6279 (1.7842) acc 54.6875 (55.9069) (mean 56.1794 many 56.1794 med nan few nan) lr 3.4549e-03 elapsed 0:10:42 eta 0:03:08\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [540/625] time 0.266 (0.266) data 0.000 (0.001) loss 1.6279 (1.7842) acc 54.6875 (55.9069) (mean 56.1794 many 56.1794 med nan few nan) lr 3.4549e-03 elapsed 0:10:42 eta 0:03:08\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [560/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.9385 (1.7206) acc 59.3750 (59.7645) (mean 59.1537 many 59.1537 med nan few nan) lr 3.4549e-03 elapsed 0:10:47 eta 0:03:03\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [560/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.9385 (1.7206) acc 59.3750 (59.7645) (mean 59.1537 many 59.1537 med nan few nan) lr 3.4549e-03 elapsed 0:10:47 eta 0:03:03\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [580/625] time 0.266 (0.266) data 0.000 (0.001) loss 1.4676 (1.7795) acc 62.5000 (56.2676) (mean 56.3766 many 56.3766 med nan few nan) lr 3.4549e-03 elapsed 0:10:53 eta 0:02:58\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [580/625] time 0.266 (0.266) data 0.000 (0.001) loss 1.4676 (1.7795) acc 62.5000 (56.2676) (mean 56.3766 many 56.3766 med nan few nan) lr 3.4549e-03 elapsed 0:10:53 eta 0:02:58\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [600/625] time 0.264 (0.266) data 0.000 (0.001) loss 1.6595 (1.7106) acc 59.3750 (59.6667) (mean 59.6985 many 59.6985 med nan few nan) lr 3.4549e-03 elapsed 0:10:58 eta 0:02:52\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [600/625] time 0.264 (0.266) data 0.000 (0.001) loss 1.6595 (1.7106) acc 59.3750 (59.6667) (mean 59.6985 many 59.6985 med nan few nan) lr 3.4549e-03 elapsed 0:10:58 eta 0:02:52\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [620/625] time 0.262 (0.266) data 0.000 (0.001) loss 1.6641 (1.7372) acc 57.8125 (59.3507) (mean 59.1763 many 59.1763 med nan few nan) lr 3.4549e-03 elapsed 0:11:04 eta 0:02:47\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [620/625] time 0.262 (0.266) data 0.000 (0.001) loss 1.6641 (1.7372) acc 57.8125 (59.3507) (mean 59.1763 many 59.1763 med nan few nan) lr 3.4549e-03 elapsed 0:11:04 eta 0:02:47\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [625/625] time 0.265 (0.266) data 0.000 (0.001) loss 1.6153 (1.7151) acc 62.5000 (60.3144) (mean 59.9833 many 59.9833 med nan few nan) lr 3.4549e-03 elapsed 0:11:05 eta 0:02:46\u001b[0m\n",
      "\u001b[37mepoch [4/5] batch [625/625] time 0.265 (0.266) data 0.000 (0.001) loss 1.6153 (1.7151) acc 62.5000 (60.3144) (mean 59.9833 many 59.9833 med nan few nan) lr 3.4549e-03 elapsed 0:11:05 eta 0:02:46\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [20/625] time 0.274 (0.266) data 0.000 (0.001) loss 1.4734 (1.7139) acc 56.2500 (58.2944) (mean 58.3345 many 58.3345 med nan few nan) lr 9.5492e-04 elapsed 0:11:11 eta 0:02:41\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [20/625] time 0.274 (0.266) data 0.000 (0.001) loss 1.4734 (1.7139) acc 56.2500 (58.2944) (mean 58.3345 many 58.3345 med nan few nan) lr 9.5492e-04 elapsed 0:11:11 eta 0:02:41\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [40/625] time 0.263 (0.266) data 0.000 (0.001) loss 1.7383 (1.7498) acc 51.5625 (57.1323) (mean 57.4232 many 57.4232 med nan few nan) lr 9.5492e-04 elapsed 0:11:16 eta 0:02:35\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [40/625] time 0.263 (0.266) data 0.000 (0.001) loss 1.7383 (1.7498) acc 51.5625 (57.1323) (mean 57.4232 many 57.4232 med nan few nan) lr 9.5492e-04 elapsed 0:11:16 eta 0:02:35\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [60/625] time 0.265 (0.266) data 0.002 (0.001) loss 1.9030 (1.7380) acc 56.2500 (58.6608) (mean 58.0912 many 58.0912 med nan few nan) lr 9.5492e-04 elapsed 0:11:22 eta 0:02:30\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [60/625] time 0.265 (0.266) data 0.002 (0.001) loss 1.9030 (1.7380) acc 56.2500 (58.6608) (mean 58.0912 many 58.0912 med nan few nan) lr 9.5492e-04 elapsed 0:11:22 eta 0:02:30\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [80/625] time 0.264 (0.266) data 0.000 (0.001) loss 1.9125 (1.7858) acc 50.0000 (56.3140) (mean 56.6588 many 56.6588 med nan few nan) lr 9.5492e-04 elapsed 0:11:27 eta 0:02:25\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [80/625] time 0.264 (0.266) data 0.000 (0.001) loss 1.9125 (1.7858) acc 50.0000 (56.3140) (mean 56.6588 many 56.6588 med nan few nan) lr 9.5492e-04 elapsed 0:11:27 eta 0:02:25\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [100/625] time 0.266 (0.266) data 0.000 (0.001) loss 1.7355 (1.7494) acc 57.8125 (57.9460) (mean 58.1913 many 58.1913 med nan few nan) lr 9.5492e-04 elapsed 0:11:32 eta 0:02:19\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [100/625] time 0.266 (0.266) data 0.000 (0.001) loss 1.7355 (1.7494) acc 57.8125 (57.9460) (mean 58.1913 many 58.1913 med nan few nan) lr 9.5492e-04 elapsed 0:11:32 eta 0:02:19\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [120/625] time 0.266 (0.266) data 0.000 (0.001) loss 1.8987 (1.7705) acc 62.5000 (58.7674) (mean 58.4314 many 58.4314 med nan few nan) lr 9.5492e-04 elapsed 0:11:38 eta 0:02:14\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [120/625] time 0.266 (0.266) data 0.000 (0.001) loss 1.8987 (1.7705) acc 62.5000 (58.7674) (mean 58.4314 many 58.4314 med nan few nan) lr 9.5492e-04 elapsed 0:11:38 eta 0:02:14\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [140/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.8949 (1.7836) acc 54.6875 (56.7084) (mean 57.1881 many 57.1881 med nan few nan) lr 9.5492e-04 elapsed 0:11:43 eta 0:02:09\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [140/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.8949 (1.7836) acc 54.6875 (56.7084) (mean 57.1881 many 57.1881 med nan few nan) lr 9.5492e-04 elapsed 0:11:43 eta 0:02:09\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [160/625] time 0.273 (0.266) data 0.002 (0.001) loss 1.5572 (1.7498) acc 62.5000 (58.3545) (mean 58.6854 many 58.6854 med nan few nan) lr 9.5492e-04 elapsed 0:11:48 eta 0:02:03\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [160/625] time 0.273 (0.266) data 0.002 (0.001) loss 1.5572 (1.7498) acc 62.5000 (58.3545) (mean 58.6854 many 58.6854 med nan few nan) lr 9.5492e-04 elapsed 0:11:48 eta 0:02:03\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [180/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.7798 (1.7302) acc 57.8125 (58.2359) (mean 57.9682 many 57.9682 med nan few nan) lr 9.5492e-04 elapsed 0:11:54 eta 0:01:58\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [180/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.7798 (1.7302) acc 57.8125 (58.2359) (mean 57.9682 many 57.9682 med nan few nan) lr 9.5492e-04 elapsed 0:11:54 eta 0:01:58\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [200/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.8172 (1.7154) acc 59.3750 (59.0617) (mean 58.9614 many 58.9614 med nan few nan) lr 9.5492e-04 elapsed 0:11:59 eta 0:01:53\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [200/625] time 0.268 (0.266) data 0.000 (0.001) loss 1.8172 (1.7154) acc 59.3750 (59.0617) (mean 58.9614 many 58.9614 med nan few nan) lr 9.5492e-04 elapsed 0:11:59 eta 0:01:53\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [220/625] time 0.264 (0.266) data 0.000 (0.001) loss 1.8519 (1.7766) acc 53.1250 (56.8189) (mean 57.0150 many 57.0150 med nan few nan) lr 9.5492e-04 elapsed 0:12:04 eta 0:01:47\u001b[0m\n",
      "\u001b[37mepoch [5/5] batch [220/625] time 0.264 (0.266) data 0.000 (0.001) loss 1.8519 (1.7766) acc 53.1250 (56.8189) (mean 57.0150 many 57.0150 med nan few nan) lr 9.5492e-04 elapsed 0:12:04 eta 0:01:47\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "RUN_TRAINING = True\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    trainer.train()\n",
    "else:\n",
    "    print(\"Skipping training; existing weights will be evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7483de0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m\n",
      "================================================================================\n",
      "                                Evaluating model                                \n",
      "================================================================================\n",
      "\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/125 [00:00<?]\u001b[0m\n",
      "\u001b[37mEvaluate on the test set\u001b[0m\n",
      "  0%|          | 0/125 [00:00<?]\u001b[0m\n",
      " 10%|9         | 12/125 [00:08<01:22]\u001b[0m\n",
      " 10%|9         | 12/125 [00:08<01:22]\u001b[0m\n",
      " 19%|#9        | 24/125 [00:17<01:12]\u001b[0m\n",
      " 19%|#9        | 24/125 [00:17<01:12]\u001b[0m\n",
      " 29%|##8       | 36/125 [00:25<01:03]\u001b[0m\n",
      " 29%|##8       | 36/125 [00:25<01:03]\u001b[0m\n",
      " 38%|###8      | 48/125 [00:34<00:54]\u001b[0m\n",
      " 38%|###8      | 48/125 [00:34<00:54]\u001b[0m\n",
      " 48%|####8     | 60/125 [00:42<00:46]\u001b[0m\n",
      " 48%|####8     | 60/125 [00:42<00:46]\u001b[0m\n",
      " 58%|#####7    | 72/125 [00:51<00:37]\u001b[0m\n",
      " 58%|#####7    | 72/125 [00:51<00:37]\u001b[0m\n",
      " 67%|######7   | 84/125 [01:00<00:29]\u001b[0m\n",
      " 67%|######7   | 84/125 [01:00<00:29]\u001b[0m\n",
      " 77%|#######6  | 96/125 [01:08<00:20]\u001b[0m\n",
      " 77%|#######6  | 96/125 [01:08<00:20]\u001b[0m\n",
      " 86%|########6 | 108/125 [01:17<00:12]\u001b[0m\n",
      " 86%|########6 | 108/125 [01:17<00:12]\u001b[0m\n",
      " 96%|#########6| 120/125 [01:26<00:03]\u001b[0m\n",
      " 96%|#########6| 120/125 [01:26<00:03]\u001b[0m\n",
      "100%|##########| 125/125 [01:29<00:00]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 8,000\n",
      "* correct: 6,119\n",
      "* accuracy: 76.5%\n",
      "* error: 23.5%\n",
      "* macro_f1: 76.1%\u001b[0m\n",
      "100%|##########| 125/125 [01:29<00:00]\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 8,000\n",
      "* correct: 6,119\n",
      "* accuracy: 76.5%\n",
      "* error: 23.5%\n",
      "* macro_f1: 76.1%\u001b[0m\n",
      "\u001b[37m* class acc: [89. 90. 83. 74. 40. 77. 76. 72. 87. 95. 63. 72. 90. 83. 86. 83. 88. 96.\n",
      " 81. 87. 90. 86. 85. 75. 84. 66. 64. 37. 85. 74. 73. 83. 56. 67. 80. 80.\n",
      " 77. 88. 71. 91. 70. 90. 63. 84. 67. 34. 90. 60. 96. 89. 60. 81. 77. 97.\n",
      " 93. 43. 87. 87. 94. 62. 88. 81. 86. 30. 66. 82. 68. 57. 98. 91. 90. 92.\n",
      " 49. 73. 27. 72. 88. 77. 82. 74.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 27.0%\n",
      "* hmean_acc: 71.3%\n",
      "* gmean_acc: 74.3%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 76.5%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 76.5%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 76.4875, 'error_rate': 23.512500000000003, 'macro_f1': 76.05082932438947, 'worst_case_acc': 27.0, 'hmean_acc': np.float64(71.26913273696587), 'gmean_acc': np.float64(74.28104883662563), 'many_acc': np.float64(76.4875), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(76.4875)})\u001b[0m\n",
      "\u001b[37m* class acc: [89. 90. 83. 74. 40. 77. 76. 72. 87. 95. 63. 72. 90. 83. 86. 83. 88. 96.\n",
      " 81. 87. 90. 86. 85. 75. 84. 66. 64. 37. 85. 74. 73. 83. 56. 67. 80. 80.\n",
      " 77. 88. 71. 91. 70. 90. 63. 84. 67. 34. 90. 60. 96. 89. 60. 81. 77. 97.\n",
      " 93. 43. 87. 87. 94. 62. 88. 81. 86. 30. 66. 82. 68. 57. 98. 91. 90. 92.\n",
      " 49. 73. 27. 72. 88. 77. 82. 74.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 27.0%\n",
      "* hmean_acc: 71.3%\n",
      "* gmean_acc: 74.3%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 76.5%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 76.5%\u001b[0m\n",
      "\u001b[37mOrderedDict({'accuracy': 76.4875, 'error_rate': 23.512500000000003, 'macro_f1': 76.05082932438947, 'worst_case_acc': 27.0, 'hmean_acc': np.float64(71.26913273696587), 'gmean_acc': np.float64(74.28104883662563), 'many_acc': np.float64(76.4875), 'med_acc': nan, 'few_acc': np.float64(nan), 'mean_acc': np.float64(76.4875)})\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 8,000\n",
      "* correct: 6,119\n",
      "* accuracy: 76.5%\n",
      "* error: 23.5%\n",
      "* macro_f1: 76.1%\u001b[0m\n",
      "\u001b[37m* class acc: [89. 90. 83. 74. 40. 77. 76. 72. 87. 95. 63. 72. 90. 83. 86. 83. 88. 96.\n",
      " 81. 87. 90. 86. 85. 75. 84. 66. 64. 37. 85. 74. 73. 83. 56. 67. 80. 80.\n",
      " 77. 88. 71. 91. 70. 90. 63. 84. 67. 34. 90. 60. 96. 89. 60. 81. 77. 97.\n",
      " 93. 43. 87. 87. 94. 62. 88. 81. 86. 30. 66. 82. 68. 57. 98. 91. 90. 92.\n",
      " 49. 73. 27. 72. 88. 77. 82. 74.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 27.0%\n",
      "* hmean_acc: 71.3%\n",
      "* gmean_acc: 74.3%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 76.5%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 76.5%\u001b[0m\n",
      "\u001b[37m=> result\n",
      "* total: 8,000\n",
      "* correct: 6,119\n",
      "* accuracy: 76.5%\n",
      "* error: 23.5%\n",
      "* macro_f1: 76.1%\u001b[0m\n",
      "\u001b[37m* class acc: [89. 90. 83. 74. 40. 77. 76. 72. 87. 95. 63. 72. 90. 83. 86. 83. 88. 96.\n",
      " 81. 87. 90. 86. 85. 75. 84. 66. 64. 37. 85. 74. 73. 83. 56. 67. 80. 80.\n",
      " 77. 88. 71. 91. 70. 90. 63. 84. 67. 34. 90. 60. 96. 89. 60. 81. 77. 97.\n",
      " 93. 43. 87. 87. 94. 62. 88. 81. 86. 30. 66. 82. 68. 57. 98. 91. 90. 92.\n",
      " 49. 73. 27. 72. 88. 77. 82. 74.]\u001b[0m\n",
      "\u001b[37m* worst_case_acc: 27.0%\n",
      "* hmean_acc: 71.3%\n",
      "* gmean_acc: 74.3%\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\u001b[0m\n",
      "\u001b[37m* many: 76.5%  med: nan%  few: nan%\u001b[0m\n",
      "\u001b[37m* average: 76.5%\u001b[0m\n",
      "\u001b[31m/tmp/ipython-input-647530208.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp_enabled):\u001b[0m\n",
      "\u001b[31m/tmp/ipython-input-647530208.py:219: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=amp_enabled):\u001b[0m\n",
      "\u001b[37mid_accuracy: 76.4875\u001b[0m\n",
      "\u001b[37mid_accuracy: 76.4875\u001b[0m\n",
      "\u001b[37mmany_acc: 76.4875\u001b[0m\n",
      "\u001b[37mmed_acc: nan\u001b[0m\n",
      "\u001b[37mfew_acc: nan\u001b[0m\n",
      "\u001b[37mauroc: 0.7157\u001b[0m\n",
      "\u001b[37maupr: 0.9133\u001b[0m\n",
      "\u001b[37mfpr@95tpr: 0.8925\u001b[0m\n",
      "\u001b[37mthreshold@95tpr: 0.1461\u001b[0m\n",
      "\u001b[37mid_mean: 0.6160\u001b[0m\n",
      "\u001b[37mid_std: 0.2828\u001b[0m\n",
      "\u001b[37mood_mean: 0.4021\u001b[0m\n",
      "\u001b[37mood_std: 0.2256\u001b[0m\n",
      "\u001b[37mmany_acc: 76.4875\u001b[0m\n",
      "\u001b[37mmed_acc: nan\u001b[0m\n",
      "\u001b[37mfew_acc: nan\u001b[0m\n",
      "\u001b[37mauroc: 0.7157\u001b[0m\n",
      "\u001b[37maupr: 0.9133\u001b[0m\n",
      "\u001b[37mfpr@95tpr: 0.8925\u001b[0m\n",
      "\u001b[37mthreshold@95tpr: 0.1461\u001b[0m\n",
      "\u001b[37mid_mean: 0.6160\u001b[0m\n",
      "\u001b[37mid_std: 0.2828\u001b[0m\n",
      "\u001b[37mood_mean: 0.4021\u001b[0m\n",
      "\u001b[37mood_std: 0.2256\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "id_acc_scalar = float(trainer.test())\n",
    "id_metrics = trainer.evaluator.evaluate()\n",
    "ood_metrics = trainer.evaluate_ood()\n",
    "\n",
    "summary = {\n",
    "    \"id_accuracy\": id_acc_scalar,\n",
    "    \"many_acc\": id_metrics.get(\"many_acc\"),\n",
    "    \"med_acc\": id_metrics.get(\"med_acc\"),\n",
    "    \"few_acc\": id_metrics.get(\"few_acc\"),\n",
    "}\n",
    "summary.update(ood_metrics)\n",
    "\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    elif value is None:\n",
    "        print(f\"{key}: N/A\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d373c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
